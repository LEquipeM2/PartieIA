{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thouria/.local/lib/python3.10/site-packages/pyannote/audio/core/io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('code/')\n",
    "from pyannote.database import registry\n",
    "from nb_functions import *\n",
    "from pyannote.metrics.diarization import DiarizationErrorRate\n",
    "from pyannote.audio import Model,Pipeline\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "HF_TOKEN = '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'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du dataset de finetuning avec les données annotées\n",
    "\n",
    "generate_dataset calcule les segments de confiance faible et crée un nouveau de donnée pour fine-tuner\n",
    "$$ generate\\_dataset(x\\_train\\_file\\_path, dataset\\_path, filename\\_path, all\\_uem\\_filename , pipeline, mode, method,threshold=0.5, window\\_size=5, annotated\\_ratio=0.15)$$\n",
    "- x_train_file_path : le chemin vers le fichier .txt recençant les noms des fichiers composant le jeu d'entrainement\n",
    "- dataset_path : le chemin vers le dossier racine du dataset\n",
    "- filename_path : le chemin et nom du fichier .txt qui sera créée pour contenir les noms des fichiers composant le jeu de fine-tuning\n",
    "- all_uem_filename : le nom du fichier qui contient l'ensembles des timelines et scores de confiance\n",
    "- mode : 'dataset' ou 'sample' pour spécifier si on veut X% du dataset ou X% des samples\n",
    "- methode : 'random' ou 'worst' pour spécifier si on veut sélectionner les segments de confiance faible ou aléatoirement\n",
    "- pipeline : segmentation.SoftSpeakerSegmentation(segmentation=model_seg, use_auth_token=HF_TOKEN)\n",
    "- threshold : le seuil de confiance minimal\n",
    "- window_size : taille de la fenêtre glissante\n",
    "- annotated_ratio : le pourcentage de données à annoter pour le fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ad280c7fd547539e8dfa94476927e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='Database:', options=('AMI', 'Msdwild'), value='AMI')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4605fc5915144fd809a8756f275604e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='Generate new dataset:', index=1, options=('Yes', 'No'), value='No')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8e0f2abe091467aa87aa4acbf168093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='Evaluate the pretrained pipeline :', index=1, options=('Yes', 'No'), value='No')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3a2cd2921384e0a87b6519837ee35b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Validate', icon='check', style=ButtonStyle(), tooltip='Validate')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6951da0a7954ffd83517072d8a3568f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='Mode:', options=('sample', 'dataset'), value='sample')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef52ab2cd89b4d06a96ab59d43571c83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='Method:', index=1, options=('random', 'lowest'), value='lowest')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7eb821016604ef1ac15f4584390d593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.3, description='Annotated ratio :', layout=Layout(align_items='flex-start', display='flex', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9a88bb9d111432bada4efcc86691af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.5, description='Threshold of the confidence (float) :', layout=Layout(align_items='flex-star…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf57054de3346d98f682695638b8bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=7.5, description='Size of the sliding window (seconds):', layout=Layout(align_items='flex-star…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3102e8b33a74e4eaf19229ca8f8e34c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Generate dataset', icon='check', style=ButtonStyle(), tooltip='Generate dataset')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating soft segmentation and low confidence segments for the fine tuning set\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c68406b60bd4d16b5abecbb31ed1f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "database_wildget, widget_generate_new_ds, eval_widget, widget_validate= display_choices()\n",
    "display(database_wildget, widget_generate_new_ds, eval_widget, widget_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'AMI-SDM.SpeakerDiarization.only_words' found in /home/thouria/Documents/PartieIA/datasets-pyannote/ami/pyannote/database.yml does not define the 'scope' of speaker labels (file, database, or global). Setting it to 'file'.\n",
      "'AMI-SDM.SpeakerDiarization.mini' found in /home/thouria/Documents/PartieIA/datasets-pyannote/ami/pyannote/database.yml does not define the 'scope' of speaker labels (file, database, or global). Setting it to 'file'.\n",
      "'AMI.SpeakerDiarization.only_words' found in /home/thouria/Documents/PartieIA/datasets-pyannote/ami/pyannote/database.yml does not define the 'scope' of speaker labels (file, database, or global). Setting it to 'file'.\n",
      "'AMI.SpeakerDiarization.mini' found in /home/thouria/Documents/PartieIA/datasets-pyannote/ami/pyannote/database.yml does not define the 'scope' of speaker labels (file, database, or global). Setting it to 'file'.\n",
      "'AMI.SpeakerDiarization.word_and_vocalsounds' found in /home/thouria/Documents/PartieIA/datasets-pyannote/ami/pyannote/database.yml does not define the 'scope' of speaker labels (file, database, or global). Setting it to 'file'.\n",
      "Checking that the 'annotation' key is present in all train files...\n",
      "Checking that the 'annotation' key is present in all test files...\n"
     ]
    }
   ],
   "source": [
    "evaluate = eval_widget.value == \"Yes\"\n",
    "database = database_wildget.value\n",
    "if database == \"AMI\":\n",
    "    protocol = \"AMI.SpeakerDiarization.mini\"\n",
    "    yaml_path = \"datasets-pyannote/ami/pyannote/database.yml\"\n",
    "elif database == \"Msdwild\":\n",
    "    protocol = \"MSDWILD.SpeakerDiarization.CustomFew\"\n",
    "    yaml_path = \"datasets-pyannote/msdwild/database.yml\"\n",
    "    \n",
    "registry.load_database(yaml_path)\n",
    "dataset = registry.get_protocol(protocol)\n",
    "print(\"Checking that the 'annotation' key is present in all train files...\")\n",
    "for file in dataset.train():\n",
    "   assert \"annotation\" in file\n",
    "print(\"Checking that the 'annotation' key is present in all test files...\")\n",
    "for file in dataset.test():\n",
    "  assert \"annotation\" in file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation des prédictions avant Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7eade4fa5094cfbaabb1a738da5f3af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating the pretrained pipeline: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The pretrained pipeline reaches a Diarization Error Rate (DER) of 19.2% on test set.\n"
     ]
    }
   ],
   "source": [
    "pretrained_pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token=HF_TOKEN)\n",
    "pretrained_pipeline.to(torch.device(device))\n",
    "torch.cuda.empty_cache()\n",
    "metric_pretrained = DiarizationErrorRate()\n",
    "if evaluate:\n",
    "    for file in tqdm(dataset.test(), desc=\"Evaluating the pretrained pipeline\"):\n",
    "        if file[\"database\"] == \"AMI\":\n",
    "            path_to_wav = \"datasets-pyannote/ami/wav/\"\n",
    "            suffixe = \".Mix-Headset\"\n",
    "        elif file[\"database\"] == \"MSDWILD\":\n",
    "            path_to_wav = \"datasets-pyannote/msdwild/wav/\"\n",
    "            suffixe = \"\"\n",
    "        file[\"pretrained pipeline\"] = pretrained_pipeline(path_to_wav+file[\"uri\"]+suffixe+\".wav\")\n",
    "        metric_pretrained(file[\"annotation\"], file[\"pretrained pipeline\"],uem=file[\"annotated\"],detailed=True)\n",
    "    print(f\"\\nThe pretrained pipeline reaches a Diarization Error Rate (DER) of {100 * abs(metric_pretrained):.1f}% on test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning Segmentation3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code récupéré sur https://github.com/pyannote/pyannote-audio/blob/develop/tutorials/adapting_pretrained_pipeline.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name              </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">      In sizes </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">                                   Out sizes </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ sincnet           │ SincNet          │ 42.6 K │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> [1, 1, 80000] </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                                [1, 60, 293] </span>│\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ lstm              │ LSTM             │  1.4 M │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">  [1, 293, 60] </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> [[1, 293, 256], [[8, 1, 128], [8, 1, 128]]] </span>│\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ linear            │ ModuleList       │ 49.4 K │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">             ? </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                                           ? </span>│\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ classifier        │ Linear           │    903 │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> [1, 293, 128] </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                                 [1, 293, 7] </span>│\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ activation        │ LogSoftmax       │      0 │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   [1, 293, 7] </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                                 [1, 293, 7] </span>│\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>│ powerset          │ Powerset         │      0 │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">             ? </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                                           ? </span>│\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>│ validation_metric │ MetricCollection │      0 │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">             ? </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                                           ? </span>│\n",
       "└───┴───────────────────┴──────────────────┴────────┴───────────────┴─────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName             \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35m     In sizes\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35m                                  Out sizes\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ sincnet           │ SincNet          │ 42.6 K │\u001b[37m \u001b[0m\u001b[37m[1, 1, 80000]\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m                               [1, 60, 293]\u001b[0m\u001b[37m \u001b[0m│\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ lstm              │ LSTM             │  1.4 M │\u001b[37m \u001b[0m\u001b[37m [1, 293, 60]\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m[[1, 293, 256], [[8, 1, 128], [8, 1, 128]]]\u001b[0m\u001b[37m \u001b[0m│\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ linear            │ ModuleList       │ 49.4 K │\u001b[37m \u001b[0m\u001b[37m            ?\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m                                          ?\u001b[0m\u001b[37m \u001b[0m│\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ classifier        │ Linear           │    903 │\u001b[37m \u001b[0m\u001b[37m[1, 293, 128]\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m                                [1, 293, 7]\u001b[0m\u001b[37m \u001b[0m│\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ activation        │ LogSoftmax       │      0 │\u001b[37m \u001b[0m\u001b[37m  [1, 293, 7]\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m                                [1, 293, 7]\u001b[0m\u001b[37m \u001b[0m│\n",
       "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ powerset          │ Powerset         │      0 │\u001b[37m \u001b[0m\u001b[37m            ?\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m                                          ?\u001b[0m\u001b[37m \u001b[0m│\n",
       "│\u001b[2m \u001b[0m\u001b[2m6\u001b[0m\u001b[2m \u001b[0m│ validation_metric │ MetricCollection │      0 │\u001b[37m \u001b[0m\u001b[37m            ?\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m                                          ?\u001b[0m\u001b[37m \u001b[0m│\n",
       "└───┴───────────────────┴──────────────────┴────────┴───────────────┴─────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 1.5 M                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 1.5 M                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 5                                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 1.5 M                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 1.5 M                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 5                                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c192b5f7fcd452991a7c87a0f16a94b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from types import MethodType\n",
    "from torch.optim import Adam, SGD\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    RichProgressBar,\n",
    ")\n",
    "\n",
    "model_seg = \"pyannote/segmentation-3.0\"\n",
    "from pyannote.audio.tasks import Segmentation\n",
    "\n",
    "model = Model.from_pretrained(model_seg, use_auth_token=HF_TOKEN)\n",
    "model.task = Segmentation(dataset, duration=5.0, max_speakers_per_chunk=3, max_speakers_per_frame=2)\n",
    "model.setup(\"fit\")\n",
    "\n",
    "def configure_optimizers(self):\n",
    "    return Adam(self.parameters(), lr=5e-4)\n",
    "\n",
    "model.configure_optimizers = MethodType(configure_optimizers, model)\n",
    "\n",
    "monitor, direction = model.task.val_monitor\n",
    "checkpoint = ModelCheckpoint(\n",
    "    monitor=monitor,\n",
    "    mode=direction,\n",
    "    save_top_k=1,\n",
    "    every_n_epochs=1,\n",
    "    save_last=False,\n",
    "    save_weights_only=False,\n",
    "    filename=\"{epoch}\",\n",
    "    verbose=False,\n",
    ")\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=monitor,\n",
    "    mode=direction,\n",
    "    min_delta=0.0,\n",
    "    patience=10,\n",
    "    strict=True,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "callbacks = [RichProgressBar(), checkpoint, early_stopping]\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "trainer = Trainer(accelerator=\"gpu\",\n",
    "                  callbacks=callbacks,\n",
    "                  max_epochs=25,\n",
    "                  gradient_clip_val=0.5)\n",
    "                  \n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37306ff890a546ed9b445199cc15f203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating the finetuned pipeline: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The finetuned pipeline reaches a Diarization Error Rate (DER) of 19.5% on ami test set with mode sample and method lowest.\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "finetuned_model = checkpoint.best_model_path\n",
    "# with open(\"hyperparameters.pickle\", 'rb') as handle:\n",
    "#     hparameters = pickle.load(handle)\n",
    "\n",
    "from pyannote.audio.pipelines import SpeakerDiarization\n",
    "finetuned_pipeline = SpeakerDiarization(\n",
    "    segmentation=finetuned_model,\n",
    "    embedding=pretrained_pipeline.embedding,\n",
    "    embedding_exclude_overlap=pretrained_pipeline.embedding_exclude_overlap,\n",
    "    clustering=pretrained_pipeline.klustering,\n",
    ")\n",
    "\n",
    "finetuned_pipeline.to(device)\n",
    "\n",
    "finetuned_pipeline.instantiate({\n",
    "    \"segmentation\": {\n",
    "        # \"threshold\": 0.4442333667381752,\n",
    "        \"min_duration_off\": 0.0,\n",
    "    },\n",
    "    \"clustering\": {\n",
    "        \"method\": \"centroid\",\n",
    "        \"min_cluster_size\": 15 ,\n",
    "        \"threshold\": 0.6285824248662424 if database == \"ami\" else 0.8285487153337224,\n",
    "    },\n",
    "})\n",
    "metric_finetuned = DiarizationErrorRate()\n",
    "\n",
    "for file in tqdm(dataset.test(), desc=\"Evaluating the finetuned pipeline\"):\n",
    "    if file[\"database\"] == \"AMI\":\n",
    "        path_to_wav = \"datasets-pyannote/ami/wav/\"\n",
    "        suffixe = \".Mix-Headset\"\n",
    "    elif file[\"database\"] == \"MSDWILD\":\n",
    "        path_to_wav = \"datasets-pyannote/msdwild/wav/\"\n",
    "        suffixe = \"\"\n",
    "    file[\"finetuned pipeline\"]  = finetuned_pipeline(path_to_wav+file[\"uri\"]+suffixe+\".wav\")\n",
    "    metric_finetuned(file[\"annotation\"], file[\"finetuned pipeline\"] ,uem=file[\"annotated\"],detailed=True)\n",
    "print(f\"The finetuned pipeline reaches a Diarization Error Rate (DER) of {100 * abs(metric_finetuned):.1f}% on {database} test set with mode {widget_mode.value} and method {widget_method.value}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test du modèle fine tuné sur un dataset différent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyannote.database import registry\n",
    "# #Ne pas oublier de changer le fichier train dans database.yml pour qu'il pointe vers le bon fichier de fine tuning\n",
    "# torch.cuda.empty_cache()\n",
    "# database = \"msdwild\"\n",
    "\n",
    "# if database == \"ami\":\n",
    "#     protocol = \"AMI.SpeakerDiarization.mini\"\n",
    "#     yaml_path = \"datasets-pyannote/ami/pyannote/database.yml\"\n",
    "# elif database == \"msdwild\":\n",
    "#     protocol = \"MSDWILD.SpeakerDiarization.CustomFew\"\n",
    "#     yaml_path = \"datasets-pyannote/msdwild/database.yml\"\n",
    "\n",
    "# registry.load_database(yaml_path)\n",
    "# dataset = registry.get_protocol(protocol)\n",
    "# print(\"Checking that the 'annotation' key is present in all train files...\")\n",
    "# for file in dataset.train():\n",
    "#    assert \"annotation\" in file\n",
    "# print(\"Checking that the 'annotation' key is present in all test files...\")\n",
    "# for file in dataset.test():\n",
    "#   assert \"annotation\" in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from pyannote.audio.pipelines.utils.hook import ProgressHook\n",
    "# metric_pretrained = DiarizationErrorRate()\n",
    "\n",
    "# for file in tqdm(dataset.test()):\n",
    "#     if file[\"database\"] == \"AMI\":\n",
    "#         path_to_wav = \"datasets-pyannote/ami/wav/\"\n",
    "#         suffixe = \".Mix-Headset\"\n",
    "#     elif file[\"database\"] == \"MSDWILD\":\n",
    "#         path_to_wav = \"datasets-pyannote/msdwild/wav/\"\n",
    "#         suffixe = \"\"\n",
    "\n",
    "#     file[\"finetuned pipeline\"]  = finetuned_pipeline(path_to_wav+file[\"uri\"]+suffixe+\".wav\")\n",
    "#     metric_finetuned(file[\"annotation\"], file[\"finetuned pipeline\"] ,uem=file[\"annotated\"],detailed=True)\n",
    "# print(f\"The finetuned pipeline  reaches a Diarization Error Rate (DER) of {100 * abs(metric_finetuned):.1f}% on {database} test set.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

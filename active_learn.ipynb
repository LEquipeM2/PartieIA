{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #pour colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# cd drive/MyDrive/chef\\ d\\'oeuvre/Code\n",
    "# !pip install pyannote.audio\n",
    "# !pip install pyannote.core\n",
    "# !pip install pyannotebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thouria/.local/lib/python3.10/site-packages/pyannote/audio/core/io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('code/')\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import SoftSeg as segmentation\n",
    "import wave\n",
    "from pyannote.core import Annotation,Timeline\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "from pyannote.database.util import load_rttm, load_uem\n",
    "from pyannote.core import Segment\n",
    "from pyannote.audio.pipelines.utils.hook import Hooks, ArtifactHook, TimingHook, ProgressHook\n",
    "from pyannote.audio import Pipeline, Inference\n",
    "from pyannote.audio import Model\n",
    "import pickle\n",
    "from ActiveLearning import generate_dataset\n",
    "import ipywidgets as widgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SoftSeg.SoftSpeakerSegmentation at 0x770779aaa6e0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HF_TOKEN = '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'\n",
    "model_seg = \"pyannote/segmentation-3.0\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "pretrained = Model.from_pretrained(\"pyannote/segmentation-3.0\", use_auth_token=HF_TOKEN)\n",
    "pipeline = segmentation.SoftSpeakerSegmentation(segmentation=model_seg, use_auth_token=HF_TOKEN)\n",
    "print(device)\n",
    "pipeline.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "database =\"ami\"\n",
    "\n",
    "if database == \"ami\":\n",
    "    x_train_path = \"datasets-pyannote/ami/lists/train.mini.txt\"\n",
    "    dataset_path = \"datasets-pyannote/ami\"\n",
    "elif database == \"msdwild\":\n",
    "    x_train_path = \"datasets-pyannote/msdwild/lists/custom1_train.txt\"\n",
    "    dataset_path = \"datasets-pyannote/msdwild\"\n",
    "\n",
    "generate_new_dataset = True\n",
    "evaluate = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du dataset de finetuning avec les données annotées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate_dataset calcule les segments de confiance faible et crée un nouveau de donnée pour fine-tuner\n",
    "$$ generate\\_dataset(x\\_train\\_file\\_path, dataset\\_path, filename\\_path, all\\_uem\\_filename , pipeline, mode, method,threshold=0.5, window\\_size=5, annotated\\_ratio=0.15)$$\n",
    "- x_train_file_path : le chemin vers le fichier .txt recençant les noms des fichiers composant le jeu d'entrainement\n",
    "- dataset_path : le chemin vers le dossier racine du dataset\n",
    "- filename_path : le chemin et nom du fichier .txt qui sera créée pour contenir les noms des fichiers composant le jeu de fine-tuning\n",
    "- all_uem_filename : le nom du fichier qui contient l'ensembles des timelines et scores de confiance\n",
    "- mode : 'dataset' ou 'sample' pour spécifier si on veut X% du dataset ou X% des samples\n",
    "- methode : 'random' ou 'worst' pour spécifier si on veut sélectionner les segments de confiance faible ou aléatoirement\n",
    "- pipeline : segmentation.SoftSpeakerSegmentation(segmentation=model_seg, use_auth_token=HF_TOKEN)\n",
    "- threshold : le seuil de confiance minimal\n",
    "- window_size : taille de la fenêtre glissante\n",
    "- annotated_ratio : le pourcentage de données à annoter pour le fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5627b6d9582c4c99b5d5bddd6dcd303d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.3, description='Annotated ratio :', layout=Layout(align_items='flex-start', display='flex', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80426a1dc35e4b9da61ab1dcba16a8a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=7.5, description='Size of the sliding window (seconds):', layout=Layout(align_items='flex-star…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30866db8207e46a8b055469d8b0089f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='Mode:', options=('sample', 'dataset'), value='sample')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c376f8b63880480ebfe9aca7ad3d3e6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='Method:', index=1, options=('random', 'lowest'), value='lowest')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "widget_mode = widgets.RadioButtons(\n",
    "    options=['sample', 'dataset'],\n",
    "    value='sample',\n",
    "    description='Mode:',\n",
    "    disabled=False\n",
    ")\n",
    "widget_method = widgets.RadioButtons(\n",
    "    options=['random', 'lowest'],\n",
    "    value='lowest',\n",
    "    description='Method:',\n",
    "    disabled=False\n",
    ")\n",
    "widget_annotated_ratio = widgets.FloatText(\n",
    "    value=0.3,\n",
    "    step =0.1,\n",
    "    description='Annotated ratio :',\n",
    "    disabled=False,\n",
    "    adaptive_height=True,\n",
    "    layout=widgets.Layout(display=\"flex\", flex_flow=\"column\", align_items=\"flex-start\", width=\"auto\", height=\"auto\"),\n",
    "    style={\"description_width\": \"initial\"}\n",
    ")\n",
    "\n",
    "widget_windows = widgets.FloatText(\n",
    "    value=7.5,\n",
    "    step =0.5,\n",
    "    description='Size of the sliding window (seconds):',\n",
    "    disabled=False,\n",
    "    adaptive_height=True,\n",
    "    layout=widgets.Layout(display=\"flex\", flex_flow=\"column\", align_items=\"flex-start\", width=\"auto\", height=\"auto\"),\n",
    "    style={\"description_width\": \"initial\"}\n",
    ")\n",
    "display(widget_annotated_ratio)\n",
    "display(widget_windows)\n",
    "display(widget_mode)\n",
    "display(widget_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating soft segmentation and low confidence segments for the fine tuning set\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "073b1a5218de4634aea1f1eee5bcb83b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuning files created in datasets-pyannote/ami/lists/fine_uem.txt\n"
     ]
    }
   ],
   "source": [
    "#Genere les fichiers de fine-tuning\n",
    "if generate_new_dataset:\n",
    "    generate_dataset(x_train_path,dataset_path,\"fine_uem.txt\" ,\"alldsample_low.uem\" ,pipeline,mode =widget_mode.value, keep_method=widget_method.value,window_size=widget_windows.value,annotated_ratio=widget_annotated_ratio.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'AMI-SDM.SpeakerDiarization.only_words' found in /home/thouria/Documents/PartieIA/datasets-pyannote/ami/pyannote/database.yml does not define the 'scope' of speaker labels (file, database, or global). Setting it to 'file'.\n",
      "'AMI-SDM.SpeakerDiarization.mini' found in /home/thouria/Documents/PartieIA/datasets-pyannote/ami/pyannote/database.yml does not define the 'scope' of speaker labels (file, database, or global). Setting it to 'file'.\n",
      "'AMI.SpeakerDiarization.only_words' found in /home/thouria/Documents/PartieIA/datasets-pyannote/ami/pyannote/database.yml does not define the 'scope' of speaker labels (file, database, or global). Setting it to 'file'.\n",
      "'AMI.SpeakerDiarization.mini' found in /home/thouria/Documents/PartieIA/datasets-pyannote/ami/pyannote/database.yml does not define the 'scope' of speaker labels (file, database, or global). Setting it to 'file'.\n",
      "'AMI.SpeakerDiarization.word_and_vocalsounds' found in /home/thouria/Documents/PartieIA/datasets-pyannote/ami/pyannote/database.yml does not define the 'scope' of speaker labels (file, database, or global). Setting it to 'file'.\n",
      "Checking that the 'annotation' key is present in all train files...\n",
      "Checking that the 'annotation' key is present in all test files...\n"
     ]
    }
   ],
   "source": [
    "from pyannote.database import registry\n",
    "#Ne pas oublier de changer le fichier train dans database.yml pour qu'il pointe vers le bon fichier de fine tuning\n",
    "if database == \"ami\":\n",
    "    protocol = \"AMI.SpeakerDiarization.mini\"\n",
    "    yaml_path = \"datasets-pyannote/ami/pyannote/database.yml\"\n",
    "elif database == \"msdwild\":\n",
    "    protocol = \"MSDWILD.SpeakerDiarization.CustomFew\"\n",
    "    yaml_path = \"datasets-pyannote/msdwild/database.yml\"\n",
    "\n",
    "registry.load_database(yaml_path)\n",
    "dataset = registry.get_protocol(protocol)\n",
    "print(\"Checking that the 'annotation' key is present in all train files...\")\n",
    "for file in dataset.train():\n",
    "   assert \"annotation\" in file\n",
    "print(\"Checking that the 'annotation' key is present in all test files...\")\n",
    "for file in dataset.test():\n",
    "  assert \"annotation\" in file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation des prédictions avant Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyannote.audio.pipelines.speaker_diarization.SpeakerDiarization at 0x7705ee9ab010>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyannote.metrics.diarization import DiarizationErrorRate\n",
    "from pyannote.audio import Model\n",
    "pretrained_pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token=HF_TOKEN)\n",
    "pretrained_pipeline.to(torch.device(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio.pipelines.utils.hook import ProgressHook\n",
    "torch.cuda.empty_cache()\n",
    "metric_pretrained = DiarizationErrorRate()\n",
    "if evaluate:\n",
    "    for file in tqdm(dataset.test(), desc=\"Evaluating the pretrained pipeline\"):\n",
    "        if file[\"database\"] == \"AMI\":\n",
    "            path_to_wav = \"datasets-pyannote/ami/wav/\"\n",
    "            suffixe = \".Mix-Headset\"\n",
    "        elif file[\"database\"] == \"MSDWILD\":\n",
    "            path_to_wav = \"datasets-pyannote/msdwild/wav/\"\n",
    "            suffixe = \"\"\n",
    "        file[\"pretrained pipeline\"] = pretrained_pipeline(path_to_wav+file[\"uri\"]+suffixe+\".wav\")\n",
    "        metric_pretrained(file[\"annotation\"], file[\"pretrained pipeline\"],uem=file[\"annotated\"],detailed=True)\n",
    "    print(f\"\\nThe pretrained pipeline reaches a Diarization Error Rate (DER) of {100 * abs(metric_pretrained):.1f}% on test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning Segmentation3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code récupéré sur https://github.com/pyannote/pyannote-audio/blob/develop/tutorials/adapting_pretrained_pipeline.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protocol AMI.SpeakerDiarization.mini does not provide the path to audio files: adding an 'audio' preprocessor for you. See pyannote.database documentation on how to do that yourself.\n",
      "Protocol AMI.SpeakerDiarization.mini does not precompute the output of torchaudio.info(): adding a 'torchaudio.info' preprocessor for you to speed up dataloaders. See pyannote.database documentation on how to do that yourself.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name              </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">      In sizes </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">                                   Out sizes </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ sincnet           │ SincNet          │ 42.6 K │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> [1, 1, 80000] </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                                [1, 60, 293] </span>│\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ lstm              │ LSTM             │  1.4 M │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">  [1, 293, 60] </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> [[1, 293, 256], [[8, 1, 128], [8, 1, 128]]] </span>│\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ linear            │ ModuleList       │ 49.4 K │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">             ? </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                                           ? </span>│\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ classifier        │ Linear           │    903 │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> [1, 293, 128] </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                                 [1, 293, 7] </span>│\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ activation        │ LogSoftmax       │      0 │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   [1, 293, 7] </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                                 [1, 293, 7] </span>│\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>│ powerset          │ Powerset         │      0 │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">             ? </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                                           ? </span>│\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>│ validation_metric │ MetricCollection │      0 │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">             ? </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                                           ? </span>│\n",
       "└───┴───────────────────┴──────────────────┴────────┴───────────────┴─────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName             \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35m     In sizes\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35m                                  Out sizes\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ sincnet           │ SincNet          │ 42.6 K │\u001b[37m \u001b[0m\u001b[37m[1, 1, 80000]\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m                               [1, 60, 293]\u001b[0m\u001b[37m \u001b[0m│\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ lstm              │ LSTM             │  1.4 M │\u001b[37m \u001b[0m\u001b[37m [1, 293, 60]\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m[[1, 293, 256], [[8, 1, 128], [8, 1, 128]]]\u001b[0m\u001b[37m \u001b[0m│\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ linear            │ ModuleList       │ 49.4 K │\u001b[37m \u001b[0m\u001b[37m            ?\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m                                          ?\u001b[0m\u001b[37m \u001b[0m│\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ classifier        │ Linear           │    903 │\u001b[37m \u001b[0m\u001b[37m[1, 293, 128]\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m                                [1, 293, 7]\u001b[0m\u001b[37m \u001b[0m│\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ activation        │ LogSoftmax       │      0 │\u001b[37m \u001b[0m\u001b[37m  [1, 293, 7]\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m                                [1, 293, 7]\u001b[0m\u001b[37m \u001b[0m│\n",
       "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ powerset          │ Powerset         │      0 │\u001b[37m \u001b[0m\u001b[37m            ?\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m                                          ?\u001b[0m\u001b[37m \u001b[0m│\n",
       "│\u001b[2m \u001b[0m\u001b[2m6\u001b[0m\u001b[2m \u001b[0m│ validation_metric │ MetricCollection │      0 │\u001b[37m \u001b[0m\u001b[37m            ?\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m                                          ?\u001b[0m\u001b[37m \u001b[0m│\n",
       "└───┴───────────────────┴──────────────────┴────────┴───────────────┴─────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 1.5 M                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 1.5 M                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 5                                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 1.5 M                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 1.5 M                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 5                                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64fa0d9e57aa4ea18f62037bb10f8ee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from types import MethodType\n",
    "from torch.optim import Adam, SGD\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    RichProgressBar,\n",
    ")\n",
    "\n",
    "\n",
    "from pyannote.audio.tasks import Segmentation\n",
    "# if database == \"ami\":\n",
    "model = Model.from_pretrained(model_seg, use_auth_token=HF_TOKEN)\n",
    "model.task = Segmentation(dataset, duration=5.0, max_speakers_per_chunk=3, max_speakers_per_frame=2)\n",
    "model.setup(\"fit\")\n",
    "# else :\n",
    "\n",
    "#     model = Model.from_pretrained(model_seg, use_auth_token=HF_TOKEN)\n",
    "#     task = Segmentation(\n",
    "#         dataset,\n",
    "#         duration=model.specifications.duration,\n",
    "#         max_num_speakers=len(model.specifications.classes),\n",
    "#         batch_size=32,\n",
    "#         num_workers=2,\n",
    "#         loss=\"bce\",\n",
    "#         vad_loss=\"bce\")\n",
    "#     model.task = task\n",
    "#     model.setup(stage=\"fit\")\n",
    "\n",
    "\n",
    "\n",
    "def configure_optimizers(self):\n",
    "    return Adam(self.parameters(), lr=5e-4)\n",
    "\n",
    "model.configure_optimizers = MethodType(configure_optimizers, model)\n",
    "\n",
    "monitor, direction = model.task.val_monitor\n",
    "checkpoint = ModelCheckpoint(\n",
    "    monitor=monitor,\n",
    "    mode=direction,\n",
    "    save_top_k=1,\n",
    "    every_n_epochs=1,\n",
    "    save_last=False,\n",
    "    save_weights_only=False,\n",
    "    filename=\"{epoch}\",\n",
    "    verbose=False,\n",
    ")\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=monitor,\n",
    "    mode=direction,\n",
    "    min_delta=0.0,\n",
    "    patience=10,\n",
    "    strict=True,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "callbacks = [RichProgressBar(), checkpoint, early_stopping]\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "trainer = Trainer(accelerator=\"gpu\",\n",
    "                  callbacks=callbacks,\n",
    "                  max_epochs=25,\n",
    "                  gradient_clip_val=0.5)\n",
    "                  \n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a97f59dcc7694d7ca9746901b60934a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating the finetuned pipeline: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The finetuned pipeline reaches a Diarization Error Rate (DER) of 19.2% on ami test set with mode dataset and method random.\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "finetuned_model = checkpoint.best_model_path\n",
    "# with open(\"hyperparameters.pickle\", 'rb') as handle:\n",
    "#     hparameters = pickle.load(handle)\n",
    "\n",
    "from pyannote.audio.pipelines import SpeakerDiarization\n",
    "finetuned_pipeline = SpeakerDiarization(\n",
    "    segmentation=finetuned_model,\n",
    "    embedding=pretrained_pipeline.embedding,\n",
    "    embedding_exclude_overlap=pretrained_pipeline.embedding_exclude_overlap,\n",
    "    clustering=pretrained_pipeline.klustering,\n",
    ")\n",
    "\n",
    "finetuned_pipeline.to(device)\n",
    "\n",
    "finetuned_pipeline.instantiate({\n",
    "    \"segmentation\": {\n",
    "        # \"threshold\": 0.4442333667381752,\n",
    "        \"min_duration_off\": 0.0,\n",
    "    },\n",
    "    \"clustering\": {\n",
    "        \"method\": \"centroid\",\n",
    "        \"min_cluster_size\": 15 ,\n",
    "        \"threshold\": 0.6285824248662424 if database == \"ami\" else 0.8285487153337224,\n",
    "    },\n",
    "})\n",
    "metric_finetuned = DiarizationErrorRate()\n",
    "\n",
    "for file in tqdm(dataset.test(), desc=\"Evaluating the finetuned pipeline\"):\n",
    "    if file[\"database\"] == \"AMI\":\n",
    "        path_to_wav = \"datasets-pyannote/ami/wav/\"\n",
    "        suffixe = \".Mix-Headset\"\n",
    "    elif file[\"database\"] == \"MSDWILD\":\n",
    "        path_to_wav = \"datasets-pyannote/msdwild/wav/\"\n",
    "        suffixe = \"\"\n",
    "    file[\"finetuned pipeline\"]  = finetuned_pipeline(path_to_wav+file[\"uri\"]+suffixe+\".wav\")\n",
    "    metric_finetuned(file[\"annotation\"], file[\"finetuned pipeline\"] ,uem=file[\"annotated\"],detailed=True)\n",
    "print(f\"The finetuned pipeline reaches a Diarization Error Rate (DER) of {100 * abs(metric_finetuned):.1f}% on {database} test set with mode {widget_mode.value} and method {widget_method.value}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test du modèle fine tuné sur un dataset différent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyannote.database import registry\n",
    "# #Ne pas oublier de changer le fichier train dans database.yml pour qu'il pointe vers le bon fichier de fine tuning\n",
    "# torch.cuda.empty_cache()\n",
    "# database = \"msdwild\"\n",
    "\n",
    "# if database == \"ami\":\n",
    "#     protocol = \"AMI.SpeakerDiarization.mini\"\n",
    "#     yaml_path = \"datasets-pyannote/ami/pyannote/database.yml\"\n",
    "# elif database == \"msdwild\":\n",
    "#     protocol = \"MSDWILD.SpeakerDiarization.CustomFew\"\n",
    "#     yaml_path = \"datasets-pyannote/msdwild/database.yml\"\n",
    "\n",
    "# registry.load_database(yaml_path)\n",
    "# dataset = registry.get_protocol(protocol)\n",
    "# print(\"Checking that the 'annotation' key is present in all train files...\")\n",
    "# for file in dataset.train():\n",
    "#    assert \"annotation\" in file\n",
    "# print(\"Checking that the 'annotation' key is present in all test files...\")\n",
    "# for file in dataset.test():\n",
    "#   assert \"annotation\" in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from pyannote.audio.pipelines.utils.hook import ProgressHook\n",
    "# metric_pretrained = DiarizationErrorRate()\n",
    "\n",
    "# for file in tqdm(dataset.test()):\n",
    "#     if file[\"database\"] == \"AMI\":\n",
    "#         path_to_wav = \"datasets-pyannote/ami/wav/\"\n",
    "#         suffixe = \".Mix-Headset\"\n",
    "#     elif file[\"database\"] == \"MSDWILD\":\n",
    "#         path_to_wav = \"datasets-pyannote/msdwild/wav/\"\n",
    "#         suffixe = \"\"\n",
    "\n",
    "#     file[\"finetuned pipeline\"]  = finetuned_pipeline(path_to_wav+file[\"uri\"]+suffixe+\".wav\")\n",
    "#     metric_finetuned(file[\"annotation\"], file[\"finetuned pipeline\"] ,uem=file[\"annotated\"],detailed=True)\n",
    "# print(f\"The finetuned pipeline  reaches a Diarization Error Rate (DER) of {100 * abs(metric_finetuned):.1f}% on {database} test set.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('code/')\n",
    "from pyannote.database import registry\n",
    "from nb_functions import *\n",
    "from pyannote.metrics.diarization import DiarizationErrorRate\n",
    "from pyannote.audio import Model,Pipeline\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "HF_TOKEN = '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'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du dataset de finetuning avec les données annotées\n",
    "\n",
    "generate_dataset calcule les segments de confiance faible et crée un nouveau de donnée pour fine-tuner\n",
    "$$ generate\\_dataset(x\\_train\\_file\\_path, dataset\\_path, filename\\_path, all\\_uem\\_filename , pipeline, mode, method,threshold=0.5, window\\_size=5, annotated\\_ratio=0.15)$$\n",
    "- x_train_file_path : le chemin vers le fichier .txt recençant les noms des fichiers composant le jeu d'entrainement\n",
    "- dataset_path : le chemin vers le dossier racine du dataset\n",
    "- filename_path : le chemin et nom du fichier .txt qui sera créée pour contenir les noms des fichiers composant le jeu de fine-tuning\n",
    "- all_uem_filename : le nom du fichier qui contient l'ensembles des timelines et scores de confiance\n",
    "- mode : 'dataset' ou 'sample' pour spécifier si on veut X% du dataset ou X% des samples\n",
    "- methode : 'random' ou 'lowest' pour spécifier si on veut sélectionner les segments de confiance faible ou aléatoirement\n",
    "- pipeline : segmentation.SoftSpeakerSegmentation(segmentation=model_seg, use_auth_token=HF_TOKEN)\n",
    "- threshold : le seuil de confiance minimal\n",
    "- window_size : taille de la fenêtre glissante\n",
    "- annotated_ratio : le pourcentage de données à annoter pour le fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_wildget, widget_generate_new_ds, eval_widget, widget_validate= display_choices()\n",
    "display(database_wildget, widget_generate_new_ds, eval_widget, widget_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate = eval_widget.value == \"Yes\"\n",
    "database = database_wildget.value\n",
    "if database == \"AMI\":\n",
    "    protocol = \"AMI.SpeakerDiarization.mini\"\n",
    "    yaml_path = \"datasets-pyannote/ami/pyannote/database.yml\"\n",
    "elif database == \"Msdwild\":\n",
    "    protocol = \"MSDWILD.SpeakerDiarization.CustomFew\"\n",
    "    yaml_path = \"datasets-pyannote/msdwild/database.yml\"\n",
    "    \n",
    "registry.load_database(yaml_path)\n",
    "dataset = registry.get_protocol(protocol)\n",
    "print(\"Checking that the 'annotation' key is present in all train files...\")\n",
    "for file in dataset.train():\n",
    "   assert \"annotation\" in file\n",
    "print(\"Checking that the 'annotation' key is present in all test files...\")\n",
    "for file in dataset.test():\n",
    "  assert \"annotation\" in file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation des prédictions avant Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token=HF_TOKEN)\n",
    "pretrained_pipeline.to(torch.device(device))\n",
    "torch.cuda.empty_cache()\n",
    "metric_pretrained = DiarizationErrorRate()\n",
    "# evaluate = True\n",
    "if evaluate:\n",
    "    for file in tqdm(dataset.test(), desc=\"Evaluating the pretrained pipeline\"):\n",
    "        if file[\"database\"] == \"AMI\":\n",
    "            path_to_wav = \"datasets-pyannote/ami/wav/\"\n",
    "            suffixe = \".Mix-Headset\"\n",
    "        elif file[\"database\"] == \"MSDWILD\":\n",
    "            path_to_wav = \"datasets-pyannote/msdwild/wav/\"\n",
    "            suffixe = \"\"\n",
    "        file[\"pretrained pipeline\"] = pretrained_pipeline(path_to_wav+file[\"uri\"]+suffixe+\".wav\")\n",
    "        metric_pretrained(file[\"annotation\"], file[\"pretrained pipeline\"],uem=file[\"annotated\"],detailed=True)\n",
    "    print(f\"\\nThe pretrained pipeline reaches a Diarization Error Rate (DER) of {100 * abs(metric_pretrained):.1f}% on test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning Segmentation3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code récupéré sur https://github.com/pyannote/pyannote-audio/blob/develop/tutorials/adapting_pretrained_pipeline.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import MethodType\n",
    "from torch.optim import Adam, SGD\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    RichProgressBar,\n",
    ")\n",
    "\n",
    "model_seg = \"pyannote/segmentation-3.0\"\n",
    "from pyannote.audio.tasks import Segmentation\n",
    "\n",
    "model = Model.from_pretrained(model_seg, use_auth_token=HF_TOKEN)\n",
    "model.task = Segmentation(dataset, duration=5.0, max_speakers_per_chunk=3, max_speakers_per_frame=2)\n",
    "model.setup(\"fit\")\n",
    "\n",
    "def configure_optimizers(self):\n",
    "    return Adam(self.parameters(), lr=5e-4)\n",
    "\n",
    "model.configure_optimizers = MethodType(configure_optimizers, model)\n",
    "\n",
    "monitor, direction = model.task.val_monitor\n",
    "checkpoint = ModelCheckpoint(\n",
    "    monitor=monitor,\n",
    "    mode=direction,\n",
    "    save_top_k=1,\n",
    "    every_n_epochs=1,\n",
    "    save_last=False,\n",
    "    save_weights_only=False,\n",
    "    filename=\"{epoch}\",\n",
    "    verbose=False,\n",
    ")\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=monitor,\n",
    "    mode=direction,\n",
    "    min_delta=0.0,\n",
    "    patience=10,\n",
    "    strict=True,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "callbacks = [RichProgressBar(), checkpoint, early_stopping]\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "trainer = Trainer(accelerator=\"gpu\",\n",
    "                  callbacks=callbacks,\n",
    "                  max_epochs=25,\n",
    "                  gradient_clip_val=0.5)\n",
    "                  \n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "finetuned_model = checkpoint.best_model_path\n",
    "# with open(\"hyperparameters.pickle\", 'rb') as handle:\n",
    "#     hparameters = pickle.load(handle)\n",
    "\n",
    "from pyannote.audio.pipelines import SpeakerDiarization\n",
    "finetuned_pipeline = SpeakerDiarization(\n",
    "    segmentation=finetuned_model,\n",
    "    embedding=pretrained_pipeline.embedding,\n",
    "    embedding_exclude_overlap=pretrained_pipeline.embedding_exclude_overlap,\n",
    "    clustering=pretrained_pipeline.klustering,\n",
    ")\n",
    "\n",
    "finetuned_pipeline.to(device)\n",
    "\n",
    "finetuned_pipeline.instantiate({\n",
    "    \"segmentation\": {\n",
    "        # \"threshold\": 0.4442333667381752,\n",
    "        \"min_duration_off\": 0.0,\n",
    "    },\n",
    "    \"clustering\": {\n",
    "        \"method\": \"centroid\",\n",
    "        \"min_cluster_size\": 15 ,\n",
    "        \"threshold\": 0.6285824248662424 if database == \"ami\" else 0.8285487153337224,\n",
    "    },\n",
    "})\n",
    "metric_finetuned = DiarizationErrorRate()\n",
    "\n",
    "for file in tqdm(dataset.test(), desc=\"Evaluating the finetuned pipeline\"):\n",
    "    if file[\"database\"] == \"AMI\":\n",
    "        path_to_wav = \"datasets-pyannote/ami/wav/\"\n",
    "        suffixe = \".Mix-Headset\"\n",
    "    elif file[\"database\"] == \"MSDWILD\":\n",
    "        path_to_wav = \"datasets-pyannote/msdwild/wav/\"\n",
    "        suffixe = \"\"\n",
    "    file[\"finetuned pipeline\"]  = finetuned_pipeline(path_to_wav+file[\"uri\"]+suffixe+\".wav\")\n",
    "    metric_finetuned(file[\"annotation\"], file[\"finetuned pipeline\"] ,uem=file[\"annotated\"],detailed=True)\n",
    "print(f\"The finetuned pipeline reaches a Diarization Error Rate (DER) of {100 * abs(metric_finetuned):.1f}% on {database} test set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#duree totale annotee\n",
    "import os\n",
    "from pyannote.database.util import load_rttm, load_uem\n",
    "uem_folder = 'datasets-pyannote/ami/manual_uems'\n",
    "duration = 0\n",
    "uem_files = [f for f in os.listdir(uem_folder) if f.endswith('.uem')]\n",
    "for file in uem_files:\n",
    "    annotated = load_uem('datasets-pyannote/ami/manual_uems'+\"/\"+file)\n",
    "    _, annotated = annotated.popitem()\n",
    "    duration += annotated.duration()\n",
    "print(f\"The total duration of the manually annotated data is {duration/60:.1f} minutes.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test du modèle fine tuné sur un dataset différent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyannote.database import registry\n",
    "# #Ne pas oublier de changer le fichier train dans database.yml pour qu'il pointe vers le bon fichier de fine tuning\n",
    "# torch.cuda.empty_cache()\n",
    "# database = \"msdwild\"\n",
    "\n",
    "# if database == \"ami\":\n",
    "#     protocol = \"AMI.SpeakerDiarization.mini\"\n",
    "#     yaml_path = \"datasets-pyannote/ami/pyannote/database.yml\"\n",
    "# elif database == \"msdwild\":\n",
    "#     protocol = \"MSDWILD.SpeakerDiarization.CustomFew\"\n",
    "#     yaml_path = \"datasets-pyannote/msdwild/database.yml\"\n",
    "\n",
    "# registry.load_database(yaml_path)\n",
    "# dataset = registry.get_protocol(protocol)\n",
    "# print(\"Checking that the 'annotation' key is present in all train files...\")\n",
    "# for file in dataset.train():\n",
    "#    assert \"annotation\" in file\n",
    "# print(\"Checking that the 'annotation' key is present in all test files...\")\n",
    "# for file in dataset.test():\n",
    "#   assert \"annotation\" in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from pyannote.audio.pipelines.utils.hook import ProgressHook\n",
    "# metric_pretrained = DiarizationErrorRate()\n",
    "\n",
    "# for file in tqdm(dataset.test()):\n",
    "#     if file[\"database\"] == \"AMI\":\n",
    "#         path_to_wav = \"datasets-pyannote/ami/wav/\"\n",
    "#         suffixe = \".Mix-Headset\"\n",
    "#     elif file[\"database\"] == \"MSDWILD\":\n",
    "#         path_to_wav = \"datasets-pyannote/msdwild/wav/\"\n",
    "#         suffixe = \"\"\n",
    "\n",
    "#     file[\"finetuned pipeline\"]  = finetuned_pipeline(path_to_wav+file[\"uri\"]+suffixe+\".wav\")\n",
    "#     metric_finetuned(file[\"annotation\"], file[\"finetuned pipeline\"] ,uem=file[\"annotated\"],detailed=True)\n",
    "# print(f\"The finetuned pipeline  reaches a Diarization Error Rate (DER) of {100 * abs(metric_finetuned):.1f}% on {database} test set.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

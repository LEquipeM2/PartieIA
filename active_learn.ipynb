{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #pour colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# cd drive/MyDrive/chef\\ d\\'oeuvre/Code\n",
    "# !pip install pyannote.audio\n",
    "# !pip install pyannote.core\n",
    "# !pip install pyannotebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thour\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pyannote\\audio\\core\\io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n",
      "The torchaudio backend is switched to 'soundfile'. Note that 'sox_io' is not supported on Windows.\n",
      "The torchaudio backend is switched to 'soundfile'. Note that 'sox_io' is not supported on Windows.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('code/')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import SoftSeg as segmentation\n",
    "from SoftSeg import find_low_confiance_frames\n",
    "import torch\n",
    "import wave\n",
    "from pyannote.core import Annotation\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from pyannote.database.util import load_rttm, load_uem\n",
    "from pyannote.core import Segment\n",
    "from pyannote.audio.pipelines.utils.hook import Hooks, ArtifactHook, TimingHook, ProgressHook\n",
    "from pyannote.audio import Pipeline, Inference\n",
    "from pyannote.audio import Model\n",
    "import pickle\n",
    "from ActiveLearning import generate_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SoftSeg.SoftSpeakerSegmentation at 0x287fa983d70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HF_TOKEN = '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'\n",
    "model_seg = \"pyannote/segmentation-3.0\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "pretrained = Model.from_pretrained(\"pyannote/segmentation-3.0\", use_auth_token=HF_TOKEN)\n",
    "pipeline = segmentation.SoftSpeakerSegmentation(segmentation=model_seg, use_auth_token=HF_TOKEN)\n",
    "print(device)\n",
    "pipeline.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "database =\"ami\"\n",
    "\n",
    "if database == \"ami\":\n",
    "    x_train_path = \"datasets-pyannote/ami/lists/train.mini.txt\"\n",
    "    dataset_path = \"datasets-pyannote/ami\"\n",
    "elif database == \"msdwild\":\n",
    "    x_train_path = \"datasets-pyannote/msdwild/lists/custom1_train.txt\"\n",
    "    dataset_path = \"datasets-pyannote/msdwild\"\n",
    "\n",
    "generate_new_dataset = False\n",
    "evaluate = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du dataset de finetuning avec les données annotées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate_dataset calcule les segments de confiance faible et crée un nouveau de donnée pour fine-tuner\n",
    "$$ generate\\_dataset(x\\_train\\_file\\_path, dataset\\_path, filename\\_path , pipeline, threshold=0.5, window\\_size=5, annotated\\_ratio=0.15)$$\n",
    "- x_train_file_path : le chemin vers le fichier .txt recençant les noms des fichiers composant le jeu d'entrainement\n",
    "- dataset_path : le chemin vers le dossier racine du dataset\n",
    "- filename_path : le chemin et nom du fichier .txt qui sera créée pour contenir les noms des fichiers composant le jeu de fine-tuning\n",
    "- pipeline : segmentation.SoftSpeakerSegmentation(segmentation=model_seg, use_auth_token=HF_TOKEN)\n",
    "- threshold : le seuil de confiance minimal\n",
    "- window_size : taille de la fenêtre glissante\n",
    "- annotated_ratio : le pourcentage de données à annoter pour le fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating soft segmentation and low confidence segments for the fine tuning set\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f2e0d6db73e43d2b14c310287c120d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuning files created in datasets-pyannote/ami/lists/fine_uem.txt\n"
     ]
    }
   ],
   "source": [
    "#Genere les fichiers de fine-tuning\n",
    "if generate_new_dataset:\n",
    "    generate_dataset(x_train_path,dataset_path,dataset_path+\"/lists/fine_uem.txt\" , pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'AMI-SDM.SpeakerDiarization.only_words' found in D:\\Git\\PartieIA\\datasets-pyannote\\ami\\pyannote\\database.yml does not define the 'scope' of speaker labels (file, database, or global). Setting it to 'file'.\n",
      "'AMI-SDM.SpeakerDiarization.mini' found in D:\\Git\\PartieIA\\datasets-pyannote\\ami\\pyannote\\database.yml does not define the 'scope' of speaker labels (file, database, or global). Setting it to 'file'.\n",
      "'AMI.SpeakerDiarization.only_words' found in D:\\Git\\PartieIA\\datasets-pyannote\\ami\\pyannote\\database.yml does not define the 'scope' of speaker labels (file, database, or global). Setting it to 'file'.\n",
      "'AMI.SpeakerDiarization.mini' found in D:\\Git\\PartieIA\\datasets-pyannote\\ami\\pyannote\\database.yml does not define the 'scope' of speaker labels (file, database, or global). Setting it to 'file'.\n",
      "'AMI.SpeakerDiarization.word_and_vocalsounds' found in D:\\Git\\PartieIA\\datasets-pyannote\\ami\\pyannote\\database.yml does not define the 'scope' of speaker labels (file, database, or global). Setting it to 'file'.\n",
      "Checking that the 'annotation' key is present in all train files...\n",
      "Checking that the 'annotation' key is present in all test files...\n"
     ]
    }
   ],
   "source": [
    "from pyannote.database import registry\n",
    "#Ne pas oublier de changer le fichier train dans database.yml pour qu'il pointe vers le bon fichier de fine tuning\n",
    "if database == \"ami\":\n",
    "    protocol = \"AMI.SpeakerDiarization.mini\"\n",
    "    yaml_path = \"datasets-pyannote/ami/pyannote/database.yml\"\n",
    "elif database == \"msdwild\":\n",
    "    protocol = \"MSDWILD.SpeakerDiarization.CustomFew\"\n",
    "    yaml_path = \"datasets-pyannote/msdwild/database.yml\"\n",
    "\n",
    "registry.load_database(yaml_path)\n",
    "dataset = registry.get_protocol(protocol)\n",
    "print(\"Checking that the 'annotation' key is present in all train files...\")\n",
    "for file in dataset.train():\n",
    "   assert \"annotation\" in file\n",
    "print(\"Checking that the 'annotation' key is present in all test files...\")\n",
    "for file in dataset.test():\n",
    "  assert \"annotation\" in file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation des prédictions avant Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyannote.audio.pipelines.speaker_diarization.SpeakerDiarization at 0x287faa42900>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyannote.metrics.diarization import DiarizationErrorRate\n",
    "from pyannote.audio import Model\n",
    "pretrained_pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token=HF_TOKEN)\n",
    "pretrained_pipeline.to(torch.device(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6a41cfed8540d886c0d4bfad34022b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Pour contourner les problèmes de compatibilité windows/pyannote on utilise les fichiers manuellement sans pyannote-database\n",
    "x_test_path = dataset_path+\"/lists/test.mini.txt\"\n",
    "x_test = []\n",
    "y_test = []\n",
    "uem_test = []\n",
    "\n",
    "with open(x_test_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        x_test.append(dataset_path+\"/wav/\"+line[:-1]+'.Mix-Headset'+\".wav\")\n",
    "        y_test.append(dataset_path+\"/rttm/\"+line[:-1]+\".rttm\")\n",
    "        uem_test.append(dataset_path+\"/uems/\"+line[:-1]+\".uem\")\n",
    "\n",
    "dico_test={}\n",
    "for i,file in tqdm(enumerate(x_test)):\n",
    "    annotation = load_rttm(y_test[i])\n",
    "    _,annotation =  annotation.popitem()\n",
    "    annotated = load_uem(uem_test[i])\n",
    "    _,annotated = annotated.popitem()\n",
    "    dico_test[file] = {\"annotation\" : annotation, \"annotated\" : annotated}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [06:13<00:00, 124.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The pretrained pipeline reaches a Diarization Error Rate (DER) of 19.2% on test set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from pyannote.audio.pipelines.utils.hook import ProgressHook\n",
    "torch.cuda.empty_cache()\n",
    "metric_pretrained = DiarizationErrorRate()\n",
    "if evaluate:\n",
    "    for file in tqdm(dico_test):\n",
    "        # with ProgressHook() as hook:\n",
    "        # dico_test[file][\"pretrained pipeline\"] = pretrained_pipeline(file, hook=hook)\n",
    "        dico_test[file][\"pretrained pipeline\"] = pretrained_pipeline(file)\n",
    "        metric_pretrained(dico_test[file][\"annotation\"], dico_test[file][\"pretrained pipeline\"],uem=dico_test[file][\"annotated\"],detailed=True)\n",
    "    print(f\"\\nThe pretrained pipeline reaches a Diarization Error Rate (DER) of {100 * abs(metric_pretrained):.1f}% on test set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric_pretrained = DiarizationErrorRate()\n",
    "# torch.cuda.empty_cache()\n",
    "# if evaluate:\n",
    "#     for file in tqdm(dataset.test()):\n",
    "#         file[\"pretrained pipeline\"] = pretrained_pipeline(file)\n",
    "#         metric_pretrained(file[\"annotation\"], file[\"pretrained pipeline\"],uem=file[\"annotated\"],detailed=True)\n",
    "\n",
    "#     print(f\"\\nThe pretrained pipeline reaches a Diarization Error Rate (DER) of {100 * abs(metric_pretrained):.1f}% on test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning Segmentation3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code récupéré sur https://github.com/pyannote/pyannote-audio/blob/develop/tutorials/adapting_pretrained_pipeline.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio.tasks import Segmentation\n",
    "model = Model.from_pretrained(model_seg, use_auth_token=HF_TOKEN)\n",
    "task = Segmentation(\n",
    "    dataset,\n",
    "    duration=model.specifications.duration,\n",
    "    max_num_speakers=len(model.specifications.classes),\n",
    "    batch_size=32,\n",
    "    num_workers=2,\n",
    "    loss=\"bce\",\n",
    "    vad_loss=\"bce\")\n",
    "model.task = task\n",
    "model.setup(stage=\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from types import MethodType\n",
    "from torch.optim import Adam\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    RichProgressBar,\n",
    ")\n",
    "\n",
    "def configure_optimizers(self):\n",
    "    return Adam(self.parameters(), lr=1e-4)\n",
    "\n",
    "model.configure_optimizers = MethodType(configure_optimizers, model)\n",
    "\n",
    "monitor, direction = task.val_monitor\n",
    "checkpoint = ModelCheckpoint(\n",
    "    monitor=monitor,\n",
    "    mode=direction,\n",
    "    save_top_k=1,\n",
    "    every_n_epochs=1,\n",
    "    save_last=False,\n",
    "    save_weights_only=False,\n",
    "    filename=\"{epoch}\",\n",
    "    verbose=False,\n",
    ")\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=monitor,\n",
    "    mode=direction,\n",
    "    min_delta=0.0,\n",
    "    patience=10,\n",
    "    strict=True,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "callbacks = [RichProgressBar(), checkpoint, early_stopping]\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "trainer = Trainer(accelerator=\"gpu\",\n",
    "                  callbacks=callbacks,\n",
    "                  max_epochs=20,\n",
    "                  gradient_clip_val=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on a entrainé sur google colab et récupéré le modèle\n",
    "finetuned_model = Model.from_pretrained('epoch=17.ckpt')\n",
    "with open(\"hyperparameters.pickle\", 'rb') as handle:\n",
    "    hparameters = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyannote.audio.pipelines.speaker_diarization.SpeakerDiarization at 0x287faeca000>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyannote.audio.pipelines import SpeakerDiarization\n",
    "finetuned_pipeline = SpeakerDiarization(\n",
    "    segmentation=finetuned_model,\n",
    "    embedding=pretrained_pipeline.embedding,\n",
    "    embedding_exclude_overlap=pretrained_pipeline.embedding_exclude_overlap,\n",
    "    clustering=pretrained_pipeline.klustering,\n",
    ")\n",
    "\n",
    "finetuned_pipeline.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyannote.audio.pipelines.speaker_diarization.SpeakerDiarization at 0x287faeca000>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_pipeline.instantiate({\n",
    "    \"segmentation\": {\n",
    "        \"threshold\": hparameters['best_segmentation_threshold'],\n",
    "        \"min_duration_off\": 0.0,\n",
    "    },\n",
    "    \"clustering\": {\n",
    "        \"method\": \"centroid\",\n",
    "        \"min_cluster_size\": 12,\n",
    "        \"threshold\": hparameters['best_clustering_threshold'],\n",
    "    },\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67f76b3abdf24962a95d40236a24962f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The finetuned pipeline reaches a Diarization Error Rate (DER) of 20.1% on AMI.SpeakerDiarization.mini test set.\n"
     ]
    }
   ],
   "source": [
    "metric_finetuned = DiarizationErrorRate()\n",
    "torch.cuda.empty_cache()\n",
    "for file in tqdm(dico_test):\n",
    "    dico_test[file][\"finetuned pipeline\"] = finetuned_pipeline(file)\n",
    "    metric_finetuned(dico_test[file][\"annotation\"], dico_test[file][\"finetuned pipeline\"],uem=dico_test[file][\"annotated\"],detailed=True)\n",
    "\n",
    "print(f\"The finetuned pipeline reaches a Diarization Error Rate (DER) of {100 * abs(metric_finetuned):.1f}% on {database} test set.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thouria/.local/lib/python3.10/site-packages/pyannote/audio/core/io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('code/')\n",
    "from pyannote.database import registry\n",
    "from nb_functions import *\n",
    "from pyannote.metrics.diarization import DiarizationErrorRate\n",
    "from pyannote.audio import Model,Pipeline\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "HF_TOKEN = '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'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du dataset de finetuning avec les données annotées\n",
    "\n",
    "generate_dataset calcule les segments de confiance faible et crée un nouveau de donnée pour fine-tuner\n",
    "$$ generate\\_dataset(x\\_train\\_file\\_path, dataset\\_path, filename\\_path, all\\_uem\\_filename , pipeline, mode, method,threshold=0.5, window\\_size=5, annotated\\_ratio=0.15)$$\n",
    "- x_train_file_path : le chemin vers le fichier .txt recençant les noms des fichiers composant le jeu d'entrainement\n",
    "- dataset_path : le chemin vers le dossier racine du dataset\n",
    "- filename_path : le chemin et nom du fichier .txt qui sera créée pour contenir les noms des fichiers composant le jeu de fine-tuning\n",
    "- all_uem_filename : le nom du fichier qui contient l'ensembles des timelines et scores de confiance\n",
    "- mode : 'dataset' ou 'sample' pour spécifier si on veut X% du dataset ou X% des samples\n",
    "- methode : 'random' ou 'lowest' pour spécifier si on veut sélectionner les segments de confiance faible ou aléatoirement\n",
    "- pipeline : segmentation.SoftSpeakerSegmentation(segmentation=model_seg, use_auth_token=HF_TOKEN)\n",
    "- threshold : le seuil de confiance minimal\n",
    "- window_size : taille de la fenêtre glissante\n",
    "- annotated_ratio : le pourcentage de données à annoter pour le fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d7da69cc874933b949cadf2e6403c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='Database:', options=('AMI', 'Msdwild'), value='AMI')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "240f249e8bbf4319bd91492ca27ae652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='Generate new dataset:', index=1, options=('Yes', 'No'), value='No')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db1dd61d02e54a5d8f5ccdc217b386d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='Evaluate the pretrained pipeline :', index=1, options=('Yes', 'No'), value='No')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2f2b30d65774ea6b5555c54af86b595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Validate', icon='check', style=ButtonStyle(), tooltip='Validate')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "578340259abd4d16b62c97b0fcccd56d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='Mode:', options=('sample', 'dataset'), value='sample')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0909806049d40ceb62adbb4b798cebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='Method:', index=1, options=('random', 'lowest'), value='lowest')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "312d519a250b4ff7ab78bd5f8b5761bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.3, description='Annotated ratio :', layout=Layout(align_items='flex-start', display='flex', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc1ad187b874c34b41f8c73d52ab2ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.5, description='Threshold of the confidence (float) :', layout=Layout(align_items='flex-star…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "411fd705f3e942abbcb049534dab8c10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=7.5, description='Size of the sliding window (seconds):', layout=Layout(align_items='flex-star…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d3e8c4d35e4ebf9c628f16140114ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Generate dataset', icon='check', style=ButtonStyle(), tooltip='Generate dataset')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating dataset with the following parameters :\n",
      "Mode :  dataset\n",
      "Method :  lowest\n",
      "Annotated ration :  0.3\n",
      "Window size :   10.0\n",
      "Confidence threshold :  0.5\n",
      "Generating soft segmentation and low confidence segments for the fine tuning set\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfaa54da6743491eb95a6bf942ce6e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2265 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuning files created in datasets-pyannote/msdwild/lists/fine_uem.txt\n"
     ]
    }
   ],
   "source": [
    "database_wildget, widget_generate_new_ds, eval_widget, widget_validate= display_choices()\n",
    "display(database_wildget, widget_generate_new_ds, eval_widget, widget_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'MSDWILD.SpeakerDiarization.OriginalMany' found in /home/thouria/Documents/PartieIA/datasets-pyannote/msdwild/database.yml does not define the 'scope' of speaker labels (file, database, or global). Setting it to 'file'.\n",
      "'MSDWILD.SpeakerDiarization.OriginalFew' found in /home/thouria/Documents/PartieIA/datasets-pyannote/msdwild/database.yml does not define the 'scope' of speaker labels (file, database, or global). Setting it to 'file'.\n",
      "Checking that the 'annotation' key is present in all train files...\n",
      "Checking that the 'annotation' key is present in all test files...\n"
     ]
    }
   ],
   "source": [
    "evaluate = eval_widget.value == \"Yes\"\n",
    "database = database_wildget.value\n",
    "if database == \"AMI\":\n",
    "    protocol = \"AMI.SpeakerDiarization.mini\"\n",
    "    yaml_path = \"datasets-pyannote/ami/pyannote/database.yml\"\n",
    "elif database == \"Msdwild\":\n",
    "    protocol = \"MSDWILD.SpeakerDiarization.CustomFew\"\n",
    "    yaml_path = \"datasets-pyannote/msdwild/database.yml\"\n",
    "    \n",
    "registry.load_database(yaml_path)\n",
    "dataset = registry.get_protocol(protocol)\n",
    "print(\"Checking that the 'annotation' key is present in all train files...\")\n",
    "for file in dataset.train():\n",
    "   assert \"annotation\" in file\n",
    "print(\"Checking that the 'annotation' key is present in all test files...\")\n",
    "for file in dataset.test():\n",
    "  assert \"annotation\" in file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation des prédictions avant Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fdba0c920274ae69b5c3c8eb8e8a530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating the pretrained pipeline: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pretrained_pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token=HF_TOKEN)\n",
    "pretrained_pipeline.to(torch.device(device))\n",
    "torch.cuda.empty_cache()\n",
    "metric_pretrained = DiarizationErrorRate()\n",
    "# evaluate = True\n",
    "if evaluate:\n",
    "    for file in tqdm(dataset.test(), desc=\"Evaluating the pretrained pipeline\"):\n",
    "        if file[\"database\"] == \"AMI\":\n",
    "            path_to_wav = \"datasets-pyannote/ami/wav/\"\n",
    "            suffixe = \".Mix-Headset\"\n",
    "        elif file[\"database\"] == \"MSDWILD\":\n",
    "            path_to_wav = \"datasets-pyannote/msdwild/wav/\"\n",
    "            suffixe = \"\"\n",
    "        file[\"pretrained pipeline\"] = pretrained_pipeline(path_to_wav+file[\"uri\"]+suffixe+\".wav\")\n",
    "        metric_pretrained(file[\"annotation\"], file[\"pretrained pipeline\"],uem=file[\"annotated\"],detailed=True)\n",
    "    print(f\"\\nThe pretrained pipeline reaches a Diarization Error Rate (DER) of {100 * abs(metric_pretrained):.1f}% on test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning Segmentation3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code récupéré sur https://github.com/pyannote/pyannote-audio/blob/develop/tutorials/adapting_pretrained_pipeline.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import MethodType\n",
    "from torch.optim import Adam, SGD\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    RichProgressBar,\n",
    ")\n",
    "\n",
    "model_seg = \"pyannote/segmentation-3.0\"\n",
    "from pyannote.audio.tasks import Segmentation\n",
    "\n",
    "model = Model.from_pretrained(model_seg, use_auth_token=HF_TOKEN)\n",
    "model.task = Segmentation(dataset, duration=5.0, max_speakers_per_chunk=3, max_speakers_per_frame=2)\n",
    "model.setup(\"fit\")\n",
    "\n",
    "def configure_optimizers(self):\n",
    "    return Adam(self.parameters(), lr=5e-4)\n",
    "\n",
    "model.configure_optimizers = MethodType(configure_optimizers, model)\n",
    "\n",
    "monitor, direction = model.task.val_monitor\n",
    "checkpoint = ModelCheckpoint(\n",
    "    monitor=monitor,\n",
    "    mode=direction,\n",
    "    save_top_k=1,\n",
    "    every_n_epochs=1,\n",
    "    save_last=False,\n",
    "    save_weights_only=False,\n",
    "    filename=\"{epoch}\",\n",
    "    verbose=False,\n",
    ")\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=monitor,\n",
    "    mode=direction,\n",
    "    min_delta=0.0,\n",
    "    patience=10,\n",
    "    strict=True,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "callbacks = [RichProgressBar(), checkpoint, early_stopping]\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "trainer = Trainer(accelerator=\"gpu\",\n",
    "                  callbacks=callbacks,\n",
    "                  max_epochs=25,\n",
    "                  gradient_clip_val=0.5)\n",
    "                  \n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "finetuned_model = checkpoint.best_model_path\n",
    "\n",
    "from pyannote.audio.pipelines import SpeakerDiarization\n",
    "finetuned_pipeline = SpeakerDiarization(\n",
    "    segmentation=finetuned_model,\n",
    "    embedding=pretrained_pipeline.embedding,\n",
    "    embedding_exclude_overlap=pretrained_pipeline.embedding_exclude_overlap,\n",
    "    clustering=pretrained_pipeline.klustering,\n",
    ")\n",
    "\n",
    "finetuned_pipeline.to(device)\n",
    "\n",
    "finetuned_pipeline.instantiate({\n",
    "    \"segmentation\": {\n",
    "        \"min_duration_off\": 0.0,\n",
    "    },\n",
    "    \"clustering\": {\n",
    "        \"method\": \"centroid\",\n",
    "        \"min_cluster_size\": 15 ,\n",
    "        \"threshold\": 0.6285824248662424 if database == \"ami\" else 0.8285487153337224,\n",
    "    },\n",
    "})\n",
    "metric_finetuned = DiarizationErrorRate()\n",
    "\n",
    "for file in tqdm(dataset.test(), desc=\"Evaluating the finetuned pipeline\"):\n",
    "    if file[\"database\"] == \"AMI\":\n",
    "        path_to_wav = \"datasets-pyannote/ami/wav/\"\n",
    "        suffixe = \".Mix-Headset\"\n",
    "    elif file[\"database\"] == \"MSDWILD\":\n",
    "        path_to_wav = \"datasets-pyannote/msdwild/wav/\"\n",
    "        suffixe = \"\"\n",
    "    file[\"finetuned pipeline\"]  = finetuned_pipeline(path_to_wav+file[\"uri\"]+suffixe+\".wav\")\n",
    "    metric_finetuned(file[\"annotation\"], file[\"finetuned pipeline\"] ,uem=file[\"annotated\"],detailed=True)\n",
    "print(f\"The finetuned pipeline reaches a Diarization Error Rate (DER) of {100 * abs(metric_finetuned):.1f}% on {database} test set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#duree totale annotee\n",
    "import os\n",
    "from pyannote.database.util import load_rttm, load_uem\n",
    "uem_folder = 'datasets-pyannote/ami/manual_uems'\n",
    "duration = 0\n",
    "uem_files = [f for f in os.listdir(uem_folder) if f.endswith('.uem')]\n",
    "for file in uem_files:\n",
    "    annotated = load_uem('datasets-pyannote/ami/manual_uems'+\"/\"+file)\n",
    "    _, annotated = annotated.popitem()\n",
    "    duration += annotated.duration()\n",
    "print(f\"The total duration of the manually annotated data is {duration/60:.1f} minutes.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test du modèle fine tuné sur un dataset différent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyannote.database import registry\n",
    "# #Ne pas oublier de changer le fichier train dans database.yml pour qu'il pointe vers le bon fichier de fine tuning\n",
    "# torch.cuda.empty_cache()\n",
    "# database = \"msdwild\"\n",
    "\n",
    "# if database == \"ami\":\n",
    "#     protocol = \"AMI.SpeakerDiarization.mini\"\n",
    "#     yaml_path = \"datasets-pyannote/ami/pyannote/database.yml\"\n",
    "# elif database == \"msdwild\":\n",
    "#     protocol = \"MSDWILD.SpeakerDiarization.CustomFew\"\n",
    "#     yaml_path = \"datasets-pyannote/msdwild/database.yml\"\n",
    "\n",
    "# registry.load_database(yaml_path)\n",
    "# dataset = registry.get_protocol(protocol)\n",
    "# print(\"Checking that the 'annotation' key is present in all train files...\")\n",
    "# for file in dataset.train():\n",
    "#    assert \"annotation\" in file\n",
    "# print(\"Checking that the 'annotation' key is present in all test files...\")\n",
    "# for file in dataset.test():\n",
    "#   assert \"annotation\" in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from pyannote.audio.pipelines.utils.hook import ProgressHook\n",
    "# metric_pretrained = DiarizationErrorRate()\n",
    "\n",
    "# for file in tqdm(dataset.test()):\n",
    "#     if file[\"database\"] == \"AMI\":\n",
    "#         path_to_wav = \"datasets-pyannote/ami/wav/\"\n",
    "#         suffixe = \".Mix-Headset\"\n",
    "#     elif file[\"database\"] == \"MSDWILD\":\n",
    "#         path_to_wav = \"datasets-pyannote/msdwild/wav/\"\n",
    "#         suffixe = \"\"\n",
    "\n",
    "#     file[\"finetuned pipeline\"]  = finetuned_pipeline(path_to_wav+file[\"uri\"]+suffixe+\".wav\")\n",
    "#     metric_finetuned(file[\"annotation\"], file[\"finetuned pipeline\"] ,uem=file[\"annotated\"],detailed=True)\n",
    "# print(f\"The finetuned pipeline  reaches a Diarization Error Rate (DER) of {100 * abs(metric_finetuned):.1f}% on {database} test set.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

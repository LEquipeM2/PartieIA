{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thouria/.local/lib/python3.10/site-packages/pyannote/audio/core/io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('code/')\n",
    "from pyannote.database import registry\n",
    "from nb_functions import *\n",
    "from pyannote.metrics.diarization import DiarizationErrorRate\n",
    "from pyannote.audio import Model,Pipeline\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "HF_TOKEN = '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'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du dataset de finetuning avec les données annotées\n",
    "\n",
    "generate_dataset calcule les segments de confiance faible et crée un nouveau de donnée pour fine-tuner\n",
    "$$ generate\\_dataset(x\\_train\\_file\\_path, dataset\\_path, filename\\_path, all\\_uem\\_filename , pipeline, mode, method,threshold=0.5, window\\_size=5, annotated\\_ratio=0.15)$$\n",
    "- x_train_file_path : le chemin vers le fichier .txt recençant les noms des fichiers composant le jeu d'entrainement\n",
    "- dataset_path : le chemin vers le dossier racine du dataset\n",
    "- filename_path : le chemin et nom du fichier .txt qui sera créée pour contenir les noms des fichiers composant le jeu de fine-tuning\n",
    "- all_uem_filename : le nom du fichier qui contient l'ensembles des timelines et scores de confiance\n",
    "- mode : 'dataset' ou 'sample' pour spécifier si on veut X% du dataset ou X% des samples\n",
    "- methode : 'random' ou 'lowest' pour spécifier si on veut sélectionner les segments de confiance faible ou aléatoirement\n",
    "- pipeline : segmentation.SoftSpeakerSegmentation(segmentation=model_seg, use_auth_token=HF_TOKEN)\n",
    "- threshold : le seuil de confiance minimal\n",
    "- window_size : taille de la fenêtre glissante\n",
    "- annotated_ratio : le pourcentage de données à annoter pour le fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a5f1ccba9d743c191e612160e5bcecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='Database:', options=('AMI', 'Msdwild'), value='AMI')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90d71fb5a7cc4ffdad2f73379d65e4cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='Generate new dataset:', index=1, options=('Yes', 'No'), value='No')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c44dc07041ab468fba908603ec19e974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='Evaluate the pretrained pipeline :', index=1, options=('Yes', 'No'), value='No')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57dbf07d8b5a4f29854abdba9d0118a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Validate', icon='check', style=ButtonStyle(), tooltip='Validate')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d71a48605f7a45a5bc025dacbd0932f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='Mode:', options=('sample', 'dataset'), value='sample')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e2bc1bd92e44310a6650fc827d794b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='Method:', index=1, options=('random', 'lowest'), value='lowest')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8e3ef5e3c654e6992a7c09d6a1adbdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.3, description='Annotated ratio :', layout=Layout(align_items='flex-start', display='flex', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5373870c793345df8e3f2f10a24f985d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.5, description='Threshold of the confidence (float) :', layout=Layout(align_items='flex-star…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "913d998756b7460992f9f279b31a8e15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=7.5, description='Size of the sliding window (seconds):', layout=Layout(align_items='flex-star…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "272b92fd4d414a5390741a418dd16021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Generate dataset', icon='check', style=ButtonStyle(), tooltip='Generate dataset')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating dataset with the following parameters :\n",
      "Mode :  sample\n",
      "Method :  lowest\n",
      "Annotated ration :  0.3\n",
      "Window size :   10.0\n",
      "Confidence threshold :  0.6\n",
      "datasets-pyannote/ami/manual_rttm  deleted\n",
      "datasets-pyannote/ami/manual_uems  deleted\n",
      "Generating soft segmentation and low confidence segments for the fine tuning set\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "511c89f1f78c420d8c3d8512f635933f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuning files created in datasets-pyannote/ami/lists/fine_uem.txt\n"
     ]
    }
   ],
   "source": [
    "database_wildget, widget_generate_new_ds, eval_widget, widget_validate= display_choices()\n",
    "display(database_wildget, widget_generate_new_ds, eval_widget, widget_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'AMI-SDM.SpeakerDiarization.only_words' found in /home/thouria/Documents/PartieIA/datasets-pyannote/ami/pyannote/database.yml does not define the 'scope' of speaker labels (file, database, or global). Setting it to 'file'.\n",
      "'AMI-SDM.SpeakerDiarization.mini' found in /home/thouria/Documents/PartieIA/datasets-pyannote/ami/pyannote/database.yml does not define the 'scope' of speaker labels (file, database, or global). Setting it to 'file'.\n",
      "'AMI.SpeakerDiarization.only_words' found in /home/thouria/Documents/PartieIA/datasets-pyannote/ami/pyannote/database.yml does not define the 'scope' of speaker labels (file, database, or global). Setting it to 'file'.\n",
      "'AMI.SpeakerDiarization.mini' found in /home/thouria/Documents/PartieIA/datasets-pyannote/ami/pyannote/database.yml does not define the 'scope' of speaker labels (file, database, or global). Setting it to 'file'.\n",
      "'AMI.SpeakerDiarization.word_and_vocalsounds' found in /home/thouria/Documents/PartieIA/datasets-pyannote/ami/pyannote/database.yml does not define the 'scope' of speaker labels (file, database, or global). Setting it to 'file'.\n",
      "Checking that the 'annotation' key is present in all train files...\n",
      "Checking that the 'annotation' key is present in all test files...\n"
     ]
    }
   ],
   "source": [
    "evaluate = eval_widget.value == \"Yes\"\n",
    "database = database_wildget.value\n",
    "if database == \"AMI\":\n",
    "    protocol = \"AMI.SpeakerDiarization.mini\"\n",
    "    yaml_path = \"datasets-pyannote/ami/pyannote/database.yml\"\n",
    "elif database == \"Msdwild\":\n",
    "    protocol = \"MSDWILD.SpeakerDiarization.CustomFew\"\n",
    "    yaml_path = \"datasets-pyannote/msdwild/database.yml\"\n",
    "    \n",
    "registry.load_database(yaml_path)\n",
    "dataset = registry.get_protocol(protocol)\n",
    "print(\"Checking that the 'annotation' key is present in all train files...\")\n",
    "for file in dataset.train():\n",
    "   assert \"annotation\" in file\n",
    "print(\"Checking that the 'annotation' key is present in all test files...\")\n",
    "for file in dataset.test():\n",
    "  assert \"annotation\" in file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation des prédictions avant Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb56c381104d405dbdcfd6001fd89f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating the pretrained pipeline: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The pretrained pipeline reaches a Diarization Error Rate (DER) of 19.2% on test set.\n"
     ]
    }
   ],
   "source": [
    "pretrained_pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token=HF_TOKEN)\n",
    "pretrained_pipeline.to(torch.device(device))\n",
    "torch.cuda.empty_cache()\n",
    "metric_pretrained = DiarizationErrorRate()\n",
    "# evaluate = True\n",
    "if evaluate:\n",
    "    for file in tqdm(dataset.test(), desc=\"Evaluating the pretrained pipeline\"):\n",
    "        if file[\"database\"] == \"AMI\":\n",
    "            path_to_wav = \"datasets-pyannote/ami/wav/\"\n",
    "            suffixe = \".Mix-Headset\"\n",
    "        elif file[\"database\"] == \"MSDWILD\":\n",
    "            path_to_wav = \"datasets-pyannote/msdwild/wav/\"\n",
    "            suffixe = \"\"\n",
    "        file[\"pretrained pipeline\"] = pretrained_pipeline(path_to_wav+file[\"uri\"]+suffixe+\".wav\")\n",
    "        metric_pretrained(file[\"annotation\"], file[\"pretrained pipeline\"],uem=file[\"annotated\"],detailed=True)\n",
    "    print(f\"\\nThe pretrained pipeline reaches a Diarization Error Rate (DER) of {100 * abs(metric_pretrained):.1f}% on test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning Segmentation3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code récupéré sur https://github.com/pyannote/pyannote-audio/blob/develop/tutorials/adapting_pretrained_pipeline.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protocol AMI.SpeakerDiarization.mini does not provide the path to audio files: adding an 'audio' preprocessor for you. See pyannote.database documentation on how to do that yourself.\n",
      "Protocol AMI.SpeakerDiarization.mini does not precompute the output of torchaudio.info(): adding a 'torchaudio.info' preprocessor for you to speed up dataloaders. See pyannote.database documentation on how to do that yourself.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name              </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">      In sizes </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">                                   Out sizes </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ sincnet           │ SincNet          │ 42.6 K │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> [1, 1, 80000] </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                                [1, 60, 293] </span>│\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ lstm              │ LSTM             │  1.4 M │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">  [1, 293, 60] </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> [[1, 293, 256], [[8, 1, 128], [8, 1, 128]]] </span>│\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ linear            │ ModuleList       │ 49.4 K │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">             ? </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                                           ? </span>│\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ classifier        │ Linear           │    903 │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> [1, 293, 128] </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                                 [1, 293, 7] </span>│\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ activation        │ LogSoftmax       │      0 │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">   [1, 293, 7] </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                                 [1, 293, 7] </span>│\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>│ powerset          │ Powerset         │      0 │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">             ? </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                                           ? </span>│\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>│ validation_metric │ MetricCollection │      0 │<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">             ? </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">                                           ? </span>│\n",
       "└───┴───────────────────┴──────────────────┴────────┴───────────────┴─────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName             \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35m     In sizes\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35m                                  Out sizes\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ sincnet           │ SincNet          │ 42.6 K │\u001b[37m \u001b[0m\u001b[37m[1, 1, 80000]\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m                               [1, 60, 293]\u001b[0m\u001b[37m \u001b[0m│\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ lstm              │ LSTM             │  1.4 M │\u001b[37m \u001b[0m\u001b[37m [1, 293, 60]\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m[[1, 293, 256], [[8, 1, 128], [8, 1, 128]]]\u001b[0m\u001b[37m \u001b[0m│\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ linear            │ ModuleList       │ 49.4 K │\u001b[37m \u001b[0m\u001b[37m            ?\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m                                          ?\u001b[0m\u001b[37m \u001b[0m│\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ classifier        │ Linear           │    903 │\u001b[37m \u001b[0m\u001b[37m[1, 293, 128]\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m                                [1, 293, 7]\u001b[0m\u001b[37m \u001b[0m│\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ activation        │ LogSoftmax       │      0 │\u001b[37m \u001b[0m\u001b[37m  [1, 293, 7]\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m                                [1, 293, 7]\u001b[0m\u001b[37m \u001b[0m│\n",
       "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ powerset          │ Powerset         │      0 │\u001b[37m \u001b[0m\u001b[37m            ?\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m                                          ?\u001b[0m\u001b[37m \u001b[0m│\n",
       "│\u001b[2m \u001b[0m\u001b[2m6\u001b[0m\u001b[2m \u001b[0m│ validation_metric │ MetricCollection │      0 │\u001b[37m \u001b[0m\u001b[37m            ?\u001b[0m\u001b[37m \u001b[0m│\u001b[37m \u001b[0m\u001b[37m                                          ?\u001b[0m\u001b[37m \u001b[0m│\n",
       "└───┴───────────────────┴──────────────────┴────────┴───────────────┴─────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 1.5 M                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 1.5 M                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 5                                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 1.5 M                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 1.5 M                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 5                                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7c26147c85f4884bc316a92ebc075b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from types import MethodType\n",
    "from torch.optim import Adam, SGD\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    RichProgressBar,\n",
    ")\n",
    "\n",
    "model_seg = \"pyannote/segmentation-3.0\"\n",
    "from pyannote.audio.tasks import Segmentation\n",
    "\n",
    "model = Model.from_pretrained(model_seg, use_auth_token=HF_TOKEN)\n",
    "model.task = Segmentation(dataset, duration=5.0, max_speakers_per_chunk=3, max_speakers_per_frame=2)\n",
    "model.setup(\"fit\")\n",
    "\n",
    "def configure_optimizers(self):\n",
    "    return Adam(self.parameters(), lr=5e-4)\n",
    "\n",
    "model.configure_optimizers = MethodType(configure_optimizers, model)\n",
    "\n",
    "monitor, direction = model.task.val_monitor\n",
    "checkpoint = ModelCheckpoint(\n",
    "    monitor=monitor,\n",
    "    mode=direction,\n",
    "    save_top_k=1,\n",
    "    every_n_epochs=1,\n",
    "    save_last=False,\n",
    "    save_weights_only=False,\n",
    "    filename=\"{epoch}\",\n",
    "    verbose=False,\n",
    ")\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=monitor,\n",
    "    mode=direction,\n",
    "    min_delta=0.0,\n",
    "    patience=10,\n",
    "    strict=True,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "callbacks = [RichProgressBar(), checkpoint, early_stopping]\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "trainer = Trainer(accelerator=\"gpu\",\n",
    "                  callbacks=callbacks,\n",
    "                  max_epochs=25,\n",
    "                  gradient_clip_val=0.5)\n",
    "                  \n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d154519c60c43db8d5b0fd170dc0803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating the finetuned pipeline: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "finetuned_model = checkpoint.best_model_path\n",
    "\n",
    "from pyannote.audio.pipelines import SpeakerDiarization\n",
    "finetuned_pipeline = SpeakerDiarization(\n",
    "    segmentation=finetuned_model,\n",
    "    embedding=pretrained_pipeline.embedding,\n",
    "    embedding_exclude_overlap=pretrained_pipeline.embedding_exclude_overlap,\n",
    "    clustering=pretrained_pipeline.klustering,\n",
    ")\n",
    "\n",
    "finetuned_pipeline.to(device)\n",
    "\n",
    "finetuned_pipeline.instantiate({\n",
    "    \"segmentation\": {\n",
    "        \"min_duration_off\": 0.0,\n",
    "    },\n",
    "    \"clustering\": {\n",
    "        \"method\": \"centroid\",\n",
    "        \"min_cluster_size\": 15 ,\n",
    "        \"threshold\": 0.6285824248662424 if database == \"ami\" else 0.8285487153337224,\n",
    "    },\n",
    "})\n",
    "metric_finetuned = DiarizationErrorRate()\n",
    "\n",
    "for file in tqdm(dataset.test(), desc=\"Evaluating the finetuned pipeline\"):\n",
    "    if file[\"database\"] == \"AMI\":\n",
    "        path_to_wav = \"datasets-pyannote/ami/wav/\"\n",
    "        suffixe = \".Mix-Headset\"\n",
    "    elif file[\"database\"] == \"MSDWILD\":\n",
    "        path_to_wav = \"datasets-pyannote/msdwild/wav/\"\n",
    "        suffixe = \"\"\n",
    "    file[\"finetuned pipeline\"]  = finetuned_pipeline(path_to_wav+file[\"uri\"]+suffixe+\".wav\")\n",
    "    metric_finetuned(file[\"annotation\"], file[\"finetuned pipeline\"] ,uem=file[\"annotated\"],detailed=True)\n",
    "print(f\"The finetuned pipeline reaches a Diarization Error Rate (DER) of {100 * abs(metric_finetuned):.1f}% on {database} test set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total duration of the manually annotated data is 10.5 minutes.\n"
     ]
    }
   ],
   "source": [
    "#duree totale annotee\n",
    "import os\n",
    "from pyannote.database.util import load_rttm, load_uem\n",
    "uem_folder = 'datasets-pyannote/ami/manual_uems'\n",
    "duration = 0\n",
    "uem_files = [f for f in os.listdir(uem_folder) if f.endswith('.uem')]\n",
    "for file in uem_files:\n",
    "    annotated = load_uem('datasets-pyannote/ami/manual_uems'+\"/\"+file)\n",
    "    _, annotated = annotated.popitem()\n",
    "    duration += annotated.duration()\n",
    "print(f\"The total duration of the manually annotated data is {duration:.1f} seconds.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test du modèle fine tuné sur un dataset différent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyannote.database import registry\n",
    "# #Ne pas oublier de changer le fichier train dans database.yml pour qu'il pointe vers le bon fichier de fine tuning\n",
    "# torch.cuda.empty_cache()\n",
    "# database = \"msdwild\"\n",
    "\n",
    "# if database == \"ami\":\n",
    "#     protocol = \"AMI.SpeakerDiarization.mini\"\n",
    "#     yaml_path = \"datasets-pyannote/ami/pyannote/database.yml\"\n",
    "# elif database == \"msdwild\":\n",
    "#     protocol = \"MSDWILD.SpeakerDiarization.CustomFew\"\n",
    "#     yaml_path = \"datasets-pyannote/msdwild/database.yml\"\n",
    "\n",
    "# registry.load_database(yaml_path)\n",
    "# dataset = registry.get_protocol(protocol)\n",
    "# print(\"Checking that the 'annotation' key is present in all train files...\")\n",
    "# for file in dataset.train():\n",
    "#    assert \"annotation\" in file\n",
    "# print(\"Checking that the 'annotation' key is present in all test files...\")\n",
    "# for file in dataset.test():\n",
    "#   assert \"annotation\" in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from pyannote.audio.pipelines.utils.hook import ProgressHook\n",
    "# metric_pretrained = DiarizationErrorRate()\n",
    "\n",
    "# for file in tqdm(dataset.test()):\n",
    "#     if file[\"database\"] == \"AMI\":\n",
    "#         path_to_wav = \"datasets-pyannote/ami/wav/\"\n",
    "#         suffixe = \".Mix-Headset\"\n",
    "#     elif file[\"database\"] == \"MSDWILD\":\n",
    "#         path_to_wav = \"datasets-pyannote/msdwild/wav/\"\n",
    "#         suffixe = \"\"\n",
    "\n",
    "#     file[\"finetuned pipeline\"]  = finetuned_pipeline(path_to_wav+file[\"uri\"]+suffixe+\".wav\")\n",
    "#     metric_finetuned(file[\"annotation\"], file[\"finetuned pipeline\"] ,uem=file[\"annotated\"],detailed=True)\n",
    "# print(f\"The finetuned pipeline  reaches a Diarization Error Rate (DER) of {100 * abs(metric_finetuned):.1f}% on {database} test set.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

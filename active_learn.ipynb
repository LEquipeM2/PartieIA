{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('code/')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import SoftSeg as segmentation\n",
    "import torch\n",
    "from pyannote.core import Annotation\n",
    "from pyannotebook import Pyannotebook\n",
    "from pyannote.core import Segment\n",
    "from pyannote.audio.pipelines.utils.hook import Hooks, ArtifactHook, TimingHook, ProgressHook\n",
    "\n",
    "HF_TOKEN = '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'\n",
    "model_seg = \"pyannote/segmentation-3.0\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = segmentation.SoftSpeakerSegmentation(segmentation=model_seg, use_auth_token=HF_TOKEN)\n",
    "print(device)\n",
    "pipeline.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset AMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ami = 'amicorpus'\n",
    "audios = []\n",
    "for folder in os.listdir(path_ami):\n",
    "    subfolder_path = os.path.join(path_ami, folder, os.listdir(os.path.join(path_ami, folder))[0])\n",
    "    audio_path = os.path.join(subfolder_path, os.listdir(subfolder_path)[0])\n",
    "    audios.append(audio_path.split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests sur sample.wav pour l'instant mais DS AMI disponible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test on file 0\n",
    "#file = {\"audio\": audios[0]}\n",
    "file = {\"audio\": \"EN2003a.Mix-Headset.wav\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SoftSegmentation on files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Hooks(ArtifactHook(), TimingHook(), ProgressHook()) as hook:\n",
    "    soft_segmentation: segmentation.SlidingWindowFeature = pipeline(file, hook=hook)\n",
    "file[\"artifact\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_ps_segmentation = np.exp(file['artifact']['aggregated_ps_segmentation'])\n",
    "aggregated_ps_segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_seg_agg = file['artifact']['aggregated_ps_segmentation']\n",
    "\n",
    "print(f\"type = {type(ps_seg_agg)}\")\n",
    "print(\"\")\n",
    "print(f\"type of .data = {type(ps_seg_agg.data)}\")\n",
    "print(f\"shape of .data = {ps_seg_agg.data.shape} (NUM_FRAMES, NUM_PS_CLASSES)\")\n",
    "print(\"\")\n",
    "print(f\"type of .sliding_window = {type(ps_seg_agg.sliding_window)}\")\n",
    "print(f\"duration of .sliding_window = {ps_seg_agg.sliding_window.duration:.4f} seconds\")\n",
    "print(f\"step of .sliding_window = {ps_seg_agg.sliding_window.step:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"type = {type(soft_segmentation)}\")\n",
    "print(\"\")\n",
    "print(f\"type of .data = {type(soft_segmentation.data)}\")\n",
    "print(f\"shape of .data = {soft_segmentation.data.shape} (NUM_FRAMES, NUM_PS_CLASSES)\")\n",
    "print(\"\")\n",
    "print(f\"type of .sliding_window = {type(soft_segmentation.sliding_window)}\")\n",
    "print(f\"duration of .sliding_window = {soft_segmentation.sliding_window.duration:.4f} seconds\")\n",
    "print(f\"step of .sliding_window = {soft_segmentation.sliding_window.step:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calcul des X% zones de confiance minimale\n",
    "Confiance = Probabilité maximale - Seconde Probabilité maximale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Cette fonction récupère 100% des zones de confiance faible : A MODIF POUR FAIRE X % SEULEMENT\n",
    "# def find_low_confidence_frames(slidingwindow, threshold=0.15, window_size=5.0):\n",
    "#     \"\"\"\n",
    "#     Identify frames with confidence lower than the specified threshold using a sliding window.\n",
    "\n",
    "#     Parameters:\n",
    "#     - slidingwindow: SlidingWindowFeature\n",
    "#         SlidingWindowFeature containing the aggregated PS segmentation data.\n",
    "#     - threshold: float, optional\n",
    "#         Confidence threshold, below which frames are considered low confidence.\n",
    "#     - window_size: float, optional\n",
    "#         Size of the sliding window in seconds.\n",
    "\n",
    "#     Returns:\n",
    "#     - list of int\n",
    "#         List of segments with low confidence.\n",
    "#     \"\"\"\n",
    "#     low_confidence_frames = []\n",
    "#     i = 0\n",
    "\n",
    "#     while i < len(slidingwindow.data):\n",
    "#         maxi_prob = np.max(slidingwindow.data[i])\n",
    "#         second_maxi_prob = np.sort(slidingwindow.data[i])[-2]\n",
    "#         confiance = maxi_prob - second_maxi_prob\n",
    "#         start = slidingwindow.sliding_window.start + i * slidingwindow.sliding_window.step\n",
    "#         if confiance < threshold:\n",
    "#             while (i + 1 < len(slidingwindow.data) and confiance < threshold):\n",
    "#                 i += 1\n",
    "#                 maxi_prob = np.max(slidingwindow.data[i])\n",
    "#                 second_maxi_prob = np.sort(slidingwindow.data[i])[-2]\n",
    "#                 confiance = maxi_prob - second_maxi_prob\n",
    "#             segment = Segment(start, slidingwindow.sliding_window.start + i * slidingwindow.sliding_window.step)\n",
    "#             if segment.duration >= window_size:\n",
    "#                 low_confidence_frames.append(segment)\n",
    "#         else:\n",
    "#             i += 1\n",
    "\n",
    "#     return low_confidence_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cette fonction récupère X% des zones de confiance faible\n",
    "def find_low_confiance_frames(segmentation, threshold=0.15, window_size=10.0, annotated_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Identify frames with confiance lower than the specified threshold using a sliding window.\n",
    "\n",
    "    Parameters:\n",
    "    - segmentation: segmentationFeature\n",
    "        segmentationFeature containing the aggregated PS segmentation data.\n",
    "    - threshold: float, optional\n",
    "        confiance threshold, below which frames are considered low confiance.\n",
    "    - window_size: float, optional\n",
    "        Size of the sliding window in seconds.\n",
    "\n",
    "    Returns:\n",
    "    - list of segments\n",
    "        List of segments with low confiance.\n",
    "    \"\"\"\n",
    "\n",
    "    def sliding_window(elements, window_size,step):    \n",
    "        if len(elements) <= window_size:\n",
    "            return elements\n",
    "        for i in range(0,len(elements)- window_size + 1,step):\n",
    "            yield elements[i:i+window_size]\n",
    "        #add the last window\n",
    "        if len(elements) % window_size != 0:\n",
    "            yield elements[-window_size:]\n",
    "    \n",
    "    segments = []\n",
    "    window_size = int(window_size/segmentation.sliding_window.step)\n",
    "    windows = sliding_window(segmentation.data,window_size,window_size)\n",
    "    \n",
    "    for i,window in enumerate(windows):\n",
    "        #transform nan to 0\n",
    "        window = np.nan_to_num(window)\n",
    "        maxi_prob = np.max(window)\n",
    "        second_maxi_prob = np.sort(window)[-2]\n",
    "        confiance = maxi_prob - second_maxi_prob\n",
    "        confiance_moyenne = np.mean(confiance)\n",
    "        if confiance_moyenne < threshold:\n",
    "            segments.append([(i*window_size, i*window_size+window_size), confiance_moyenne])\n",
    "\n",
    "    segments.sort(key=lambda x: x[1])\n",
    "    segments = [[Segment(segment[0][0]*segmentation.sliding_window.step, segment[0][1]*segmentation.sliding_window.step), segment[1]] for segment in segments]\n",
    "    segments = segments[:int(len(segments)*annotated_ratio)]\n",
    "    segments = [x[0] for x in segments]\n",
    "    segments.sort(key=lambda x: x.start)\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_confidence_segments = find_low_confiance_frames(soft_segmentation, threshold=0.5, window_size=5.0, annotated_ratio=0.50)\n",
    "\n",
    "for segment in low_confidence_segments:\n",
    "    print(f\"Low confidence segment: {segment}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotation with Pyannotebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annoter les fenetres récupérées où la confiance est faible\n",
    "\n",
    "# enregistrer les annotations en rttm avec pyannotebook save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widget = Pyannotebook(\"EN2003a.Mix-Headset.wav\")\n",
    "display(widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.core import Annotation\n",
    "assert isinstance(widget.annotation, Annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for segment, _, label in widget.annotation.itertracks(yield_label=True):\n",
    "    print(f\"{segment.start:.2f} {segment.end:.2f} {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.core import Annotation\n",
    "assert isinstance(widget.annotation, Annotation)\n",
    "annotation = Annotation()\n",
    "annotation[Segment(0, 10)] = \"speaker1\"\n",
    "annotation[Segment(10, 20)] = \"speaker2\"\n",
    "widget.annotation = annotation\n",
    "\n",
    "#save\n",
    "with open(\"manual.rttm\", \"w\") as rttm:\n",
    "    widget.annotation.write_rttm(rttm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widget.annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création du dataset de finetuning avec les données annotées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voir comment associer annotations et audio pour creer le dataset\n",
    "dataset = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning Segmentation3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code récupéré sur https://github.com/pyannote/pyannote-audio/blob/develop/tutorials/adapting_pretrained_pipeline.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyannote\\audio\\utils\\protocol.py:56\u001b[0m, in \u001b[0;36mcheck_protocol\u001b[1;34m(protocol)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 56\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[43mprotocol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m())\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mNotImplementedError\u001b[39;00m):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'train'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m Model\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_seg, use_auth_token\u001b[38;5;241m=\u001b[39mHF_TOKEN)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyannote\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Segmentation\n\u001b[1;32m----> 5\u001b[0m task \u001b[38;5;241m=\u001b[39m \u001b[43mSegmentation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mduration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspecifications\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_num_speakers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspecifications\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbce\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvad_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbce\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m=\u001b[39m task\n\u001b[0;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39msetup(stage\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyannote\\audio\\tasks\\segmentation\\speaker_diarization.py:146\u001b[0m, in \u001b[0;36mSpeakerDiarization.__init__\u001b[1;34m(self, protocol, duration, max_speakers_per_chunk, max_speakers_per_frame, weigh_by_cardinality, warm_up, balance, weight, batch_size, num_workers, pin_memory, augmentation, vad_loss, metric, max_num_speakers, loss)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    129\u001b[0m     protocol: SpeakerDiarizationProtocol,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    144\u001b[0m     loss: Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbce\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# deprecated\u001b[39;00m\n\u001b[0;32m    145\u001b[0m ):\n\u001b[1;32m--> 146\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mduration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarm_up\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarm_up\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpin_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpin_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43maugmentation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugmentation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(protocol, SpeakerDiarizationProtocol):\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    159\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpeakerDiarization task requires a SpeakerDiarizationProtocol.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyannote\\audio\\core\\task.py:221\u001b[0m, in \u001b[0;36mTask.__init__\u001b[1;34m(self, protocol, duration, min_duration, warm_up, batch_size, num_workers, pin_memory, augmentation, metric)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m    220\u001b[0m \u001b[38;5;66;03m# dataset\u001b[39;00m\n\u001b[1;32m--> 221\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprotocol, checks \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_protocol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_validation \u001b[38;5;241m=\u001b[39m checks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_scope \u001b[38;5;241m=\u001b[39m checks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_scope\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyannote\\audio\\utils\\protocol.py:58\u001b[0m, in \u001b[0;36mcheck_protocol\u001b[1;34m(protocol)\u001b[0m\n\u001b[0;32m     56\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(protocol\u001b[38;5;241m.\u001b[39mtrain())\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mNotImplementedError\u001b[39;00m):\n\u001b[1;32m---> 58\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProtocol \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mprotocol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not define a training set.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# does protocol provide audio keys?\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'name'"
     ]
    }
   ],
   "source": [
    "from pyannote.audio import Model\n",
    "model = Model.from_pretrained(model_seg, use_auth_token=HF_TOKEN)\n",
    "\n",
    "from pyannote.audio.tasks import Segmentation\n",
    "task = Segmentation(\n",
    "    dataset, \n",
    "    duration=model.specifications.duration, \n",
    "    max_num_speakers=len(model.specifications.classes), \n",
    "    batch_size=32,\n",
    "    num_workers=2, \n",
    "    loss=\"bce\", \n",
    "    vad_loss=\"bce\")\n",
    "model.task = task\n",
    "model.setup(stage=\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import MethodType\n",
    "from torch.optim import Adam\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    RichProgressBar,\n",
    ")\n",
    "\n",
    "def configure_optimizers(self):\n",
    "    return Adam(self.parameters(), lr=1e-4)\n",
    "\n",
    "model.configure_optimizers = MethodType(configure_optimizers, model)\n",
    "\n",
    "# we monitor diarization error rate on the validation set\n",
    "# and use to keep the best checkpoint and stop early\n",
    "monitor, direction = task.val_monitor\n",
    "checkpoint = ModelCheckpoint(\n",
    "    monitor=monitor,\n",
    "    mode=direction,\n",
    "    save_top_k=1,\n",
    "    every_n_epochs=1,\n",
    "    save_last=False,\n",
    "    save_weights_only=False,\n",
    "    filename=\"{epoch}\",\n",
    "    verbose=False,\n",
    ")\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=monitor,\n",
    "    mode=direction,\n",
    "    min_delta=0.0,\n",
    "    patience=10,\n",
    "    strict=True,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "callbacks = [RichProgressBar(), checkpoint, early_stopping]\n",
    "\n",
    "# we train for at most 20 epochs (might be shorter in case of early stopping)\n",
    "from pytorch_lightning import Trainer\n",
    "trainer = Trainer(accelerator=\"gpu\", \n",
    "                  callbacks=callbacks, \n",
    "                  max_epochs=20,\n",
    "                  gradient_clip_val=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "if not os.path.exists(\"./saved_models/fintuned\"):\n",
    "    os.makedirs(\"./saved_models/fintuned\")\n",
    "pt_save_directory = \"./saved_models/fintuned/\"\n",
    "finetuned_model = checkpoint.best_model_path\n",
    "finetuned_model.save_pretrained(pt_save_directory)\n",
    "print('Model Saved')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
